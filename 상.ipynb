{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "78a8b254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [Section 1] ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# [Part 1] ê¸°ì¡´ FFFFFFFINAL_ENS.ipynbì˜ í•µì‹¬ ì „ì²˜ë¦¬ í•¨ìˆ˜ë“¤\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "# 1. ê¸°ë³¸ ì „ì²˜ë¦¬ (ë‚˜ì´ ë³€í™˜ ë“±)\n",
    "def prepare_raw_simple(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    if \"birth_date\" in df.columns:\n",
    "        df[\"birth_date\"] = pd.to_datetime(df[\"birth_date\"])\n",
    "        # ê¸°ì¤€ì¼ì€ 2025ë…„ìœ¼ë¡œ ê°€ì • (ê¸°ì¡´ ì½”ë“œ ìœ ì§€)\n",
    "        df[\"age\"] = 2025 - df[\"birth_date\"].dt.year\n",
    "    \n",
    "    # gender ìˆ«ì ë³€í™˜ (XGBìš©, CatBoostìš©ì€ ì›ë³¸ genderë¥¼ ì“°ì§€ë§Œ ì—¬ê¸°ì„  ë¯¸ë¦¬ ë§Œë“¤ì–´ë‘ )\n",
    "    if \"gender\" in df.columns:\n",
    "        df[\"gender_int\"] = df[\"gender\"].map({\"M\": 0, \"F\": 1})\n",
    "    return df\n",
    "\n",
    "# 2. EDA Rule Features\n",
    "def add_eda_rule_features_v2(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "\n",
    "    # 1) chem_01_range\n",
    "    df['chem_01_range_str'] = pd.cut(\n",
    "        df['chem_01'], \n",
    "        bins=[-np.inf, 2.0, 4.0, np.inf], \n",
    "        labels=['Normal_Low', 'Warning_Zone', 'Danger_High']\n",
    "    ).astype(str)\n",
    "\n",
    "    chem_map = {'Normal_Low': 0, 'Warning_Zone': 1, 'Danger_High': 2}\n",
    "    df['chem_01_range'] = df['chem_01_range_str'].map(chem_map).astype(int)\n",
    "\n",
    "    # 2) trace_metal_range\n",
    "    df['trace_metal_range_str'] = pd.cut(\n",
    "        df['trace_metal'],\n",
    "        bins=[-np.inf, 90, 130, np.inf],\n",
    "        labels=['Safe', 'Caution', 'Risk']\n",
    "    ).astype(str)\n",
    "\n",
    "    metal_map = {'Safe': 0, 'Caution': 1, 'Risk': 2}\n",
    "    df['trace_metal_range'] = df['trace_metal_range_str'].map(metal_map).astype(int)\n",
    "\n",
    "    # 3) risk_segment\n",
    "    def check_ambiguous_risk(row):\n",
    "        if (row['swelling'] == 'S') and (2.0 <= row['chem_01'] < 5.0):\n",
    "            return 'Target_1_Suspect'\n",
    "        elif (row['swelling'] == 'Y') or (row['chem_01'] >= 5.0):\n",
    "            return 'High_Risk'\n",
    "        else:\n",
    "            return 'Normal'\n",
    "\n",
    "    df['risk_segment_str'] = df.apply(check_ambiguous_risk, axis=1).astype(str)\n",
    "    risk_map = {'Normal': 0, 'Target_1_Suspect': 1, 'High_Risk': 2}\n",
    "    df['risk_segment'] = df['risk_segment_str'].map(risk_map).astype(int)\n",
    "\n",
    "    return df\n",
    "\n",
    "# 3. Boosting Features\n",
    "def make_boosting_features_v2(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "\n",
    "    # ê¸°ë³¸ ìˆ˜ì¹˜í˜• íŒŒìƒ\n",
    "    high_is_healthy = [\"protein_level\", \"blood_cells\"]\n",
    "    low_is_healthy  = [\"enzyme_A\", \"enzyme_B\", \"lipid_index\", \"trace_metal\", \"clot_time\"]\n",
    "\n",
    "    def minmax(s):\n",
    "        s = s.astype(float)\n",
    "        mn, mx = s.min(), s.max()\n",
    "        if mx == mn: return pd.Series(0.5, index=s.index)\n",
    "        return (s - mn) / (mx - mn + 1e-6)\n",
    "\n",
    "    if all(col in df.columns for col in high_is_healthy + low_is_healthy):\n",
    "        df[\"health_index\"] = (\n",
    "            sum(minmax(df[col]) for col in high_is_healthy)\n",
    "            - sum(minmax(df[col]) for col in low_is_healthy)\n",
    "        )\n",
    "        df[\"health_per_stage\"] = df[\"health_index\"] / (df[\"disease_stage\"] + 1e-6)\n",
    "\n",
    "    # ìƒí˜¸ì‘ìš© / ë¹„ìœ¨ / í•©Â·ì°¨\n",
    "    df[\"chem01_trace_combo\"]   = df[\"chem_01\"] * df[\"trace_metal\"]\n",
    "    df[\"chem01_chem02_combo\"]  = df[\"chem_01\"] * df[\"chem_02\"]\n",
    "    df[\"chem01_enzymeB_combo\"] = df[\"chem_01\"] * df[\"enzyme_B\"]\n",
    "    df[\"enzyme_ratio\"]       = df[\"enzyme_A\"] / (df[\"enzyme_B\"].replace(0, np.nan) + 1e-6)\n",
    "    df[\"lipid_blood_ratio\"]  = df[\"lipid_index\"] / (df[\"blood_cells\"].replace(0, np.nan) + 1e-6)\n",
    "    df[\"chem02_trace_ratio\"] = df[\"chem_02\"] / (df[\"trace_metal\"].replace(0, np.nan) + 1e-6)\n",
    "    \n",
    "    df[\"sum_chem\"]   = df[\"chem_01\"] + df[\"chem_02\"]\n",
    "    df[\"diff_chem\"]  = df[\"chem_01\"] - df[\"chem_02\"]\n",
    "    df[\"sum_enzyme\"] = df[\"enzyme_A\"] + df[\"enzyme_B\"]\n",
    "    df[\"diff_enzyme\"] = df[\"enzyme_A\"] - df[\"enzyme_B\"]\n",
    "\n",
    "    # ì‹œê°„/ë‚˜ì´ ì •ê·œí™”\n",
    "    if \"age\" in df.columns:\n",
    "        df[\"obs_per_age\"]      = df[\"obs_days\"] / (df[\"age\"] + 1e-6)\n",
    "        df[\"behavior_per_age\"] = df[\"behavior_index\"] / (df[\"age\"] + 1e-6)\n",
    "    df[\"disease_velocity\"] = df[\"disease_stage\"] / (df[\"obs_days\"] + 1e-6)\n",
    "\n",
    "    # ì¦ìƒ ì¹´ìš´íŠ¸ & Swelling ë³€í™˜\n",
    "    symptom_cols = [\"fluid_accum\", \"organ_enlarge\", \"vascular_marks\"]\n",
    "    # replace ì‹œ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬\n",
    "    temp_symptoms = df[symptom_cols].replace({\"Y\": 1, \"N\": 0}).infer_objects(copy=False)\n",
    "    df[\"symptom_count\"] = temp_symptoms.sum(axis=1)\n",
    "\n",
    "    if \"swelling\" in df.columns:\n",
    "        df[\"swelling_ord\"] = df[\"swelling\"].map({\"N\": 0, \"S\": 1, \"Y\": 2})\n",
    "    else:\n",
    "        df[\"swelling_ord\"] = 0\n",
    "    df[\"symptom_severity\"] = df[\"symptom_count\"] + df[\"swelling_ord\"]\n",
    "    \n",
    "    return df\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# [Part 2] ğŸ”¥ NEW: Mayo Score & Risk Group (ìƒˆë¡œ ì¶”ê°€ëœ í•µì‹¬ í”¼ì²˜)\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "def add_mayo_pbc_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    \n",
    "    # 1. ë‚˜ì´(Age) ê³„ì‚° (ì´ë¯¸ prepare_raw_simpleì—ì„œ ê³„ì‚°ë¨, ì—†ìœ¼ë©´ ì•ˆì „ì¥ì¹˜)\n",
    "    if 'age' not in df.columns and 'birth_date' in df.columns:\n",
    "        df['birth_date'] = pd.to_datetime(df['birth_date'])\n",
    "        df['age'] = 2025 - df['birth_date'].dt.year\n",
    "    \n",
    "    # 2. ë¶€ì¢…(Edema) ì ìˆ˜ ë§¤í•‘\n",
    "    # N(None): 0.0, S(Slight): 0.5, Y(Severe): 1.0\n",
    "    edema_map = {'N': 0.0, 'S': 0.5, 'Y': 1.0}\n",
    "    if df['swelling'].dtype == 'object':\n",
    "        edema_score = df['swelling'].map(edema_map).fillna(0.0)\n",
    "    else:\n",
    "        # ì´ë¯¸ ìˆ«ìë¡œ ë³€í™˜ëœ ê²½ìš° (0, 1, 2) -> (0.0, 0.5, 1.0)ìœ¼ë¡œ ìŠ¤ì¼€ì¼ ì¡°ì •\n",
    "        edema_score = df['swelling_ord'] * 0.5 \n",
    "        \n",
    "    # 3. Mayo PBC Risk Score ê³µì‹ ì ìš©\n",
    "    # R = 0.871*ln(Bilirubin) - 2.53*ln(Albumin) + 0.039*Age + 2.38*ln(Prothrombin) + 0.859*Edema\n",
    "    # ë§¤í•‘: Bilirubin -> chem_01, Albumin -> protein_level, Prothrombin -> clot_time\n",
    "    \n",
    "    # ë¡œê·¸ ë³€í™˜ ì‹œ 0 ì´í•˜ ê°’ ë°©ì§€ (1e-6 ë”í•¨)\n",
    "    df['mayo_score'] = (\n",
    "        0.871 * np.log(df['chem_01'] + 1e-6) \n",
    "        - 2.53 * np.log(df['protein_level'] + 1e-6) \n",
    "        + 0.039 * df['age'] \n",
    "        + 2.38 * np.log(df['clot_time'] + 1e-6) \n",
    "        + 0.859 * edema_score\n",
    "    )\n",
    "    \n",
    "    # 4. Mayo Risk Group (ê³µì‹ ë…¼ë¬¸ ê¸°ì¤€ Cut-off ì ìš©)\n",
    "    # 4.5 ë¯¸ë§Œ: ì €ìœ„í—˜(0), 4.5~7.8: ì¤‘ë“±ë„(1), 7.8 ì´ìƒ: ê³ ìœ„í—˜(2)\n",
    "    df['mayo_risk_group'] = pd.cut(\n",
    "        df['mayo_score'], \n",
    "        bins=[-np.inf, 4.5, 7.8, np.inf], \n",
    "        labels=[0, 1, 2]\n",
    "    ).astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def add_log_features(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # ë¡œê·¸ ë³€í™˜ ëŒ€ìƒ: ë²”ìœ„ê°€ ë„“ê³  ì¹˜ìš°ì¹œ ë³€ìˆ˜ë“¤\n",
    "    # 0ì´ë‚˜ ìŒìˆ˜ê°€ ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ np.log1p (log(x+1)) ì‚¬ìš©ì´ ì•ˆì „í•¨\n",
    "    target_cols = ['enzyme_A', 'enzyme_B', 'chem_01', 'chem_02', 'trace_metal', 'obs_days']\n",
    "    \n",
    "    for col in target_cols:\n",
    "        if col in df.columns:\n",
    "            # ìŒìˆ˜ ê°’ì´ ìˆë‹¤ë©´ shift í›„ ë¡œê·¸ ë³€í™˜ (ì•ˆì „ì¥ì¹˜)\n",
    "            min_val = df[col].min()\n",
    "            if min_val < 0:\n",
    "                df[f'log_{col}'] = np.log1p(df[col] - min_val)\n",
    "            else:\n",
    "                df[f'log_{col}'] = np.log1p(df[col])\n",
    "                \n",
    "    return df\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# [Part 3] ê¸°ì¡´ ì‹¬í™” ì „ì²˜ë¦¬ í•¨ìˆ˜ë“¤ (Caution, Post-EDA, Group Stats, QBins)\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "def add_discrete_caution_features(X: pd.DataFrame) -> pd.DataFrame:\n",
    "    X = X.copy()\n",
    "    \n",
    "    X[\"stage_mid\"] = X[\"disease_stage\"].between(2, 3).astype(int)\n",
    "    X[\"beh_mid\"]   = (X[\"behavior_index\"] == 1).astype(int)\n",
    "\n",
    "    X[\"caution_symptom_zone\"] = (\n",
    "        X[\"symptom_count\"].between(1, 2) & (X[\"swelling_ord\"] >= 1)\n",
    "    ).astype(int)\n",
    "\n",
    "    X[\"caution_stage_beh\"] = (\n",
    "        (X[\"stage_mid\"] == 1) & (X[\"beh_mid\"] == 1)\n",
    "    ).astype(int)\n",
    "\n",
    "    # organ_enlargeê°€ Y/Nì¼ ìˆ˜ë„ ìˆê³  0/1ì¼ ìˆ˜ë„ ìˆìŒ\n",
    "    if X['organ_enlarge'].dtype == 'object':\n",
    "        organ_flag = (X['organ_enlarge'] == 'Y')\n",
    "    else:\n",
    "        organ_flag = (X['organ_enlarge'] == 1)\n",
    "\n",
    "    X[\"caution_organ_only\"] = (\n",
    "        organ_flag\n",
    "        & (X[\"disease_stage\"] == 1)\n",
    "        & (X[\"behavior_index\"] <= 1)\n",
    "    ).astype(int)\n",
    "\n",
    "    return X\n",
    "\n",
    "def add_post_eda_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Treatment Interaction\n",
    "    df['protein_treatment_interaction'] = df['protein_level'] * (1 + df['treatment'])\n",
    "    df['blood_treatment_interaction']   = df['blood_cells'] * (1 + df['treatment'])\n",
    "    df['chem01_treatment_interaction']  = df['chem_01'] * (1 + df['treatment'])\n",
    "    df['trace_treatment_interaction']   = df['trace_metal'] * (1 + df['treatment'])\n",
    "\n",
    "    # Coagulation Index\n",
    "    df['coagulation_index'] = df['protein_level'] / (df['clot_time'] + 1e-6)\n",
    "    df['coagulation_product'] = df['protein_level'] * df['clot_time']\n",
    "\n",
    "    # Deadly Combo\n",
    "    if df['organ_enlarge'].dtype == 'object':\n",
    "        organ_val = (df['organ_enlarge'] == 'Y').astype(int)\n",
    "    else:\n",
    "        organ_val = df['organ_enlarge']\n",
    "        \n",
    "    df['deadly_combo'] = df['swelling_ord'] * organ_val * 2\n",
    "    \n",
    "    # Obs Days Nonlinearity\n",
    "    df['log_obs_days'] = np.log1p(df['obs_days'])\n",
    "    df['obs_days_squared'] = df['obs_days'] ** 2\n",
    "\n",
    "    return df\n",
    "\n",
    "def add_group_stats_no_leakage(train: pd.DataFrame, test: pd.DataFrame) -> (pd.DataFrame, pd.DataFrame):\n",
    "    train = train.copy()\n",
    "    test = test.copy()\n",
    "    \n",
    "    bins = [60, 80, 90, 100, 130]\n",
    "    labels = [\"60-79\", \"80-89\", \"90-99\", \"100+\"]\n",
    "    \n",
    "    for df in [train, test]:\n",
    "        df[\"age_group\"] = pd.cut(df[\"age\"], bins=bins, labels=labels, right=False, include_lowest=True)\n",
    "\n",
    "    group_cols = ['disease_stage', 'gender', 'age_group']\n",
    "    agg_cols = ['enzyme_A', 'enzyme_B', 'protein_level', 'immune_index', 'lipid_index', 'blood_cells']\n",
    "\n",
    "    for group_col in group_cols:\n",
    "        for col in agg_cols:\n",
    "            train_groupby = train.groupby(group_col)[col]\n",
    "            mean_map = train_groupby.mean()\n",
    "            std_map  = train_groupby.std()\n",
    "            \n",
    "            prefix = f'{col}_by_{group_col}'\n",
    "            \n",
    "            for df_target in [train, test]:\n",
    "                mapped_mean = df_target[group_col].map(mean_map).astype(float).values\n",
    "                mapped_std  = df_target[group_col].map(std_map).astype(float).values\n",
    "                col_values  = df_target[col].astype(float).values\n",
    "                \n",
    "                df_target[f'{prefix}_diff_mean']  = col_values - mapped_mean\n",
    "                df_target[f'{prefix}_zscore']     = (col_values - mapped_mean) / (mapped_std + 1e-6)\n",
    "                df_target[f'{prefix}_ratio_mean'] = col_values / (mapped_mean + 1e-6)\n",
    "\n",
    "    return train, test\n",
    "\n",
    "def add_qbins_no_leakage(train: pd.DataFrame, test: pd.DataFrame, n_bins=4) -> (pd.DataFrame, pd.DataFrame):\n",
    "    train = train.copy()\n",
    "    test = test.copy()\n",
    "    \n",
    "    lab_cols = [\n",
    "        \"chem_01\", \"chem_02\", \"protein_level\", \"trace_metal\",\n",
    "        \"enzyme_A\", \"enzyme_B\", \"lipid_index\", \"blood_cells\", \"clot_time\",\n",
    "        \"mayo_score\" # mayo_scoreë„ êµ¬ê°„í™” ëŒ€ìƒì— ì¶”ê°€ (ê¶Œì¥)\n",
    "    ]\n",
    "    \n",
    "    for col in lab_cols:\n",
    "        if col not in train.columns: continue\n",
    "        \n",
    "        try:\n",
    "            _, bins = pd.qcut(train[col], q=n_bins, retbins=True, duplicates='drop')\n",
    "        except ValueError:\n",
    "            continue\n",
    "            \n",
    "        train[f\"{col}_qbin\"] = pd.cut(train[col], bins=bins, labels=False, include_lowest=True).fillna(-1).astype(int)\n",
    "        test[f\"{col}_qbin\"] = pd.cut(test[col], bins=bins, labels=False, include_lowest=True).fillna(-1).astype(int)\n",
    "        \n",
    "    return train, test\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# [Part 4] ì „ì²´ ì „ì²˜ë¦¬ ì‹¤í–‰ í•¨ìˆ˜ (í†µí•© íŒŒì´í”„ë¼ì¸)\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "def run_feature_engineering(train_df, test_df):\n",
    "    print(\"1. ê¸°ë³¸ ì „ì²˜ë¦¬ (ë‚˜ì´ ë³€í™˜ ë“±)...\")\n",
    "    train_df = prepare_raw_simple(train_df)\n",
    "    test_df = prepare_raw_simple(test_df)\n",
    "\n",
    "    print(\"2. EDA Rule ì ìš©...\")\n",
    "    train_df = add_eda_rule_features_v2(train_df)\n",
    "    test_df  = add_eda_rule_features_v2(test_df)\n",
    "\n",
    "    print(\"3. Boosting Feature ìƒì„±...\")\n",
    "    train_df = make_boosting_features_v2(train_df)\n",
    "    test_df  = make_boosting_features_v2(test_df)\n",
    "    \n",
    "    print(\"â­ 3-1. Mayo PBC Score & Risk Group ì¶”ê°€ (NEW)...\")\n",
    "    train_df = add_mayo_pbc_features(train_df)\n",
    "    test_df  = add_mayo_pbc_features(test_df)\n",
    "\n",
    "    print(\"3-2. Log Features ìƒì„±...\")\n",
    "    train_df = add_log_features(train_df)\n",
    "    test_df  = add_log_features(test_df)\n",
    "\n",
    "    print(\"4. Caution Feature ìƒì„±...\")\n",
    "    train_df = add_discrete_caution_features(train_df)\n",
    "    test_df  = add_discrete_caution_features(test_df)\n",
    "\n",
    "    print(\"4-1. EDA ì‹¬í™” í”¼ì²˜(Treatment Interaction ë“±) ìƒì„±...\")\n",
    "    train_df = add_post_eda_features(train_df)\n",
    "    test_df  = add_post_eda_features(test_df)\n",
    "\n",
    "    print(\"5. Group Stats (No Leakage) ìƒì„±...\")\n",
    "    train_df, test_df = add_group_stats_no_leakage(train_df, test_df)\n",
    "\n",
    "    print(\"6. Q-Binning (No Leakage) ìƒì„±...\")\n",
    "    train_df, test_df = add_qbins_no_leakage(train_df, test_df)\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    return train_df, test_df\n",
    "# ì‚¬ìš© ì˜ˆì‹œ:\n",
    "# train_df, test_df = run_feature_engineering(train_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "74ee6a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Train shape: (6500, 24)\n",
      "Original Test shape:  (1405, 23)\n",
      "\n",
      "ğŸš€ ì „ì²˜ë¦¬ ë° í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ì‹œì‘...\n",
      "1. ê¸°ë³¸ ì „ì²˜ë¦¬ (ë‚˜ì´ ë³€í™˜ ë“±)...\n",
      "2. EDA Rule ì ìš©...\n",
      "3. Boosting Feature ìƒì„±...\n",
      "â­ 3-1. Mayo PBC Score & Risk Group ì¶”ê°€ (NEW)...\n",
      "3-2. Log Features ìƒì„±...\n",
      "4. Caution Feature ìƒì„±...\n",
      "4-1. EDA ì‹¬í™” í”¼ì²˜(Treatment Interaction ë“±) ìƒì„±...\n",
      "5. Group Stats (No Leakage) ìƒì„±...\n",
      "6. Q-Binning (No Leakage) ìƒì„±...\n",
      "\n",
      "Preprocessed X_all shape: (6500, 131)\n",
      "Preprocessed X_all_test shape: (1405, 131)\n",
      "\n",
      "Applying Skew-based Scaling on 105 columns...\n",
      "âœ… Smart Scaling Complete.\n"
     ]
    }
   ],
   "source": [
    "# %% [Section 2] ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ ì‹¤í–‰\n",
    "\n",
    "# 1. ë°ì´í„° ë¡œë“œ (ê²½ë¡œë¥¼ ë³¸ì¸ í™˜ê²½ì— ë§ê²Œ ìˆ˜ì •í•˜ì„¸ìš”)\n",
    "train_path = r\"C:\\Users\\abc01\\OneDrive\\ë°”íƒ• í™”ë©´\\train.csv\"\n",
    "test_path  = r\"C:\\Users\\abc01\\OneDrive\\ë°”íƒ• í™”ë©´\\test.csv\"\n",
    "\n",
    "train_df = pd.read_csv(train_path)\n",
    "test_df  = pd.read_csv(test_path)\n",
    "\n",
    "print(f\"Original Train shape: {train_df.shape}\")\n",
    "print(f\"Original Test shape:  {test_df.shape}\")\n",
    "\n",
    "# 2. ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (Mayo Score í¬í•¨)\n",
    "# ì•ì„œ ì •ì˜í•œ run_feature_engineering í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤.\n",
    "print(\"\\nğŸš€ ì „ì²˜ë¦¬ ë° í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ì‹œì‘...\")\n",
    "train_df, test_df = run_feature_engineering(train_df, test_df)\n",
    "\n",
    "# 3. X, y ë¶„ë¦¬ ë° ë¶ˆí•„ìš” ì»¬ëŸ¼ ì œê±°\n",
    "drop_cols = [\"index\", \"name\", \"birth_date\", \"geo_code\"]\n",
    "y = train_df[\"target\"].values\n",
    "X_all = train_df.drop(columns=drop_cols + [\"target\"], errors='ignore')\n",
    "X_all_test = test_df.drop(columns=drop_cols, errors='ignore')\n",
    "\n",
    "print(f\"\\nPreprocessed X_all shape: {X_all.shape}\")\n",
    "print(f\"Preprocessed X_all_test shape: {X_all_test.shape}\")\n",
    "\n",
    "# %% [Section 3] ìŠ¤ë§ˆíŠ¸ ìŠ¤ì¼€ì¼ë§ (Skewness ê¸°ë°˜)\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, QuantileTransformer\n",
    "\n",
    "# 1. ìŠ¤ì¼€ì¼ë§ ëŒ€ìƒ ì„ ì •: ìˆœìˆ˜ ìˆ˜ì¹˜í˜• ë°ì´í„°ë§Œ (ë²”ì£¼í˜•, ë°”ì´ë„ˆë¦¬, qbin ì œì™¸)\n",
    "numeric_candidates = X_all.select_dtypes(include=['number']).columns\n",
    "\n",
    "# ìŠ¤ì¼€ì¼ë§ì—ì„œ ì œì™¸í•  ì»¬ëŸ¼ë“¤ (ì´ë¯¸ ë²”ì£¼í˜•ì´ê±°ë‚˜ ì¸ì½”ë”©ëœ ê²ƒë“¤)\n",
    "EXCLUDE_COLS = [\n",
    "    \"gender_int\", \"fluid_accum\", \"organ_enlarge\", \"vascular_marks\", \n",
    "    \"swelling\", \"treatment\", \"geo_code\", \"risk_segment\",\n",
    "    \"chem_01_range\", \"trace_metal_range\", \"swelling_ord\",\n",
    "    \"mayo_risk_group\", # â­ NEW: ìƒˆë¡œ ì¶”ê°€ëœ ë²”ì£¼í˜• í”¼ì²˜ë„ ì œì™¸\n",
    "    \"target\", \"index\"\n",
    "]\n",
    "\n",
    "# _qbinìœ¼ë¡œ ëë‚˜ëŠ” ì»¬ëŸ¼ë„ ì œì™¸ (ì´ë¯¸ êµ¬ê°„í™”ë¨)\n",
    "NUMERIC_COLS = [c for c in numeric_candidates if c not in EXCLUDE_COLS and not c.endswith('_qbin')]\n",
    "\n",
    "# 2. ìŠ¤ì¼€ì¼ë§ í•¨ìˆ˜ (ì™œë„ì— ë”°ë¼ Scaler ìë™ ì„ íƒ)\n",
    "def apply_skew_scaling(train_df, test_df, cols):\n",
    "    train_df = train_df.copy()\n",
    "    test_df = test_df.copy()\n",
    "    \n",
    "    # Train ê¸°ì¤€ Skewness ê³„ì‚°\n",
    "    skewness = train_df[cols].skew()\n",
    "    print(f\"\\nApplying Skew-based Scaling on {len(cols)} columns...\")\n",
    "    \n",
    "    for col in cols:\n",
    "        skew = abs(skewness[col])\n",
    "        \n",
    "        if skew < 0.5:\n",
    "            scaler = StandardScaler()\n",
    "        elif skew < 1.5:\n",
    "            scaler = RobustScaler()\n",
    "        else:\n",
    "            # ì™œë„ê°€ ì‹¬í•˜ë©´ ì •ê·œë¶„í¬ë¡œ ê°•ì œ ë³€í™˜\n",
    "            scaler = QuantileTransformer(output_distribution=\"normal\", random_state=42)\n",
    "            \n",
    "        train_df[col] = scaler.fit_transform(train_df[[col]]).flatten()\n",
    "        test_df[col] = scaler.transform(test_df[[col]]).flatten()\n",
    "            \n",
    "    return train_df, test_df\n",
    "\n",
    "# 3. ìŠ¤ì¼€ì¼ë§ ì ìš©\n",
    "X_all, X_all_test = apply_skew_scaling(X_all, X_all_test, NUMERIC_COLS)\n",
    "print(\"âœ… Smart Scaling Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "eac5eb78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying Skew-based Scaling on 106 columns...\n",
      "Smart Scaling Complete.\n"
     ]
    }
   ],
   "source": [
    "# %% [Section 4] ìŠ¤ë§ˆíŠ¸ ìŠ¤ì¼€ì¼ë§ (Skewness ê¸°ë°˜)\n",
    "\n",
    "# 1. ìŠ¤ì¼€ì¼ë§ ëŒ€ìƒ: ìˆœìˆ˜ ìˆ˜ì¹˜í˜• ë°ì´í„°ë§Œ (ë²”ì£¼í˜•, ë°”ì´ë„ˆë¦¬ ì œì™¸)\n",
    "numeric_candidates = X_all.select_dtypes(include=['number']).columns\n",
    "\n",
    "EXCLUDE_COLS = [\n",
    "    \"gender_int\", \"fluid_accum\", \"organ_enlarge\", \"vascular_marks\", \n",
    "    \"swelling\", \"treatment\", \"geo_code\", \"risk_segment\",\n",
    "    \"chem_01_range\", \"trace_metal_range\", \"swelling_ord\",\n",
    "    \"target\", \"index\"\n",
    "]\n",
    "\n",
    "NUMERIC_COLS = [c for c in numeric_candidates if c not in EXCLUDE_COLS and not c.endswith('_qbin')]\n",
    "\n",
    "# 2. ìŠ¤ì¼€ì¼ë§ í•¨ìˆ˜\n",
    "def apply_skew_scaling(train_df, test_df, cols):\n",
    "    train_df = train_df.copy()\n",
    "    test_df = test_df.copy()\n",
    "    \n",
    "    # Train ê¸°ì¤€ Skewness ê³„ì‚°\n",
    "    skewness = train_df[cols].skew()\n",
    "    print(f\"Applying Skew-based Scaling on {len(cols)} columns...\")\n",
    "    \n",
    "    for col in cols:\n",
    "        skew = abs(skewness[col])\n",
    "        \n",
    "        if skew < 0.5:\n",
    "            scaler = StandardScaler()\n",
    "        elif skew < 1.5:\n",
    "            scaler = RobustScaler()\n",
    "        else:\n",
    "            scaler = QuantileTransformer(output_distribution=\"normal\", random_state=42)\n",
    "            \n",
    "        train_df[col] = scaler.fit_transform(train_df[[col]]).flatten()\n",
    "        test_df[col] = scaler.transform(test_df[[col]]).flatten()\n",
    "            \n",
    "    return train_df, test_df\n",
    "\n",
    "# 3. ì ìš©\n",
    "X_all, X_all_test = apply_skew_scaling(X_all, X_all_test, NUMERIC_COLS)\n",
    "print(\"Smart Scaling Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "967fef42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ›¡ï¸ ì±”í”¼ì–¸(36) + âš”ï¸ Mayo(3)\n",
      "ğŸš€ ìµœì¢… í”¼ì²˜ ê°œìˆ˜: 39ê°œ\n",
      "   ëª©ë¡: ['symptom_severity', 'blood_cells_by_age_group_diff_mean', 'gender_int', 'lipid_index_by_gender_ratio_mean', 'chem_01_qbin', 'sum_chem', 'trace_treatment_interaction', 'enzyme_B', 'mayo_score', 'deadly_combo', 'treatment', 'blood_cells_by_disease_stage_diff_mean', 'chem01_treatment_interaction', 'chem01_chem02_combo', 'enzyme_ratio', 'lipid_index_by_disease_stage_ratio_mean', 'mayo_risk_group', 'chem_01', 'enzyme_B_by_disease_stage_diff_mean', 'symptom_count', 'health_per_stage', 'chem_01_range', 'age', 'sum_enzyme', 'enzyme_B_by_age_group_ratio_mean', 'obs_days', 'behavior_per_age', 'enzyme_B_by_disease_stage_zscore', 'lipid_index_qbin', 'blood_cells_by_gender_ratio_mean', 'mayo_score_qbin', 'blood_cells_by_gender_zscore', 'swelling_ord', 'enzyme_A_by_disease_stage_ratio_mean', 'clot_time_qbin', 'chem01_trace_combo', 'enzyme_A', 'enzyme_A_by_gender_diff_mean', 'blood_treatment_interaction']\n",
      "\n",
      "ğŸš€ [Final Check] 5-Fold CV ì‹œì‘...\n",
      "   [Fold 1] mAP: 0.68938\n",
      "   [Fold 2] mAP: 0.67352\n",
      "   [Fold 3] mAP: 0.67603\n",
      "   [Fold 4] mAP: 0.71451\n",
      "   [Fold 5] mAP: 0.73985\n",
      "   [Fold 6] mAP: 0.72321\n",
      "   [Fold 7] mAP: 0.65493\n",
      "   [Fold 8] mAP: 0.71941\n",
      "   [Fold 9] mAP: 0.76463\n",
      "   [Fold 10] mAP: 0.65523\n",
      "\n",
      "========================================\n",
      "ğŸ† ìµœì¢… í‰ê·  mAP: 0.701070\n",
      "========================================\n",
      "âœ… X_xgb ë°ì´í„°ì…‹ì´ [ì±”í”¼ì–¸+Mayo] ì¡°í•©ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤!\n"
     ]
    }
   ],
   "source": [
    "# %% [Final Fix] ì •ì˜ˆ 36ê°œ + Mayo Score ì¡°í•© ê²€ì¦\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import numpy as np\n",
    "\n",
    "# 1. ì „ì„¤ì˜ 36ê°œ í”¼ì²˜ (0.69955 ê¸°ë¡)\n",
    "CHAMPION_FEATS = [\n",
    "    'chem_01_range', 'chem01_trace_combo', 'chem_01', 'symptom_count', 'clot_time_qbin', \n",
    "    'symptom_severity', 'health_per_stage', 'obs_days', 'swelling_ord', 'gender_int', \n",
    "    'age', 'lipid_index_by_gender_ratio_mean', 'chem_01_qbin', 'enzyme_B_by_disease_stage_zscore', \n",
    "    'blood_cells_by_gender_zscore', 'enzyme_A_by_gender_diff_mean', 'chem01_treatment_interaction', \n",
    "    'deadly_combo', 'enzyme_A_by_disease_stage_ratio_mean', 'enzyme_B', 'sum_enzyme', \n",
    "    'chem01_chem02_combo', 'sum_chem', 'blood_cells_by_gender_ratio_mean', \n",
    "    'trace_treatment_interaction', 'enzyme_B_by_age_group_ratio_mean', \n",
    "    'lipid_index_by_disease_stage_ratio_mean', 'blood_cells_by_age_group_diff_mean', \n",
    "    'enzyme_A', 'enzyme_B_by_disease_stage_diff_mean', 'blood_treatment_interaction', \n",
    "    'blood_cells_by_disease_stage_diff_mean', 'enzyme_ratio', 'behavior_per_age', \n",
    "    'lipid_index_qbin', 'treatment'\n",
    "]\n",
    "\n",
    "# 2. ì—¬ê¸°ì— Mayoë§Œ ë”± ì¶”ê°€! (ë‚˜ë¨¸ì§€ ì¡ë‹¤í•œ ê±´ ë²„ë¦¼)\n",
    "# mayo_score, mayo_risk_group, ê·¸ë¦¬ê³  í˜¹ì‹œ ëª¨ë¥¼ log_mayo ë“±\n",
    "MAYO_FEATS = [c for c in X_all.columns if 'mayo' in c] \n",
    "\n",
    "FINAL_COMBINATION = list(set(CHAMPION_FEATS + MAYO_FEATS))\n",
    "FINAL_COMBINATION = [c for c in FINAL_COMBINATION if c in X_all.columns]\n",
    "\n",
    "print(f\"ğŸ›¡ï¸ ì±”í”¼ì–¸(36) + âš”ï¸ Mayo({len(MAYO_FEATS)})\")\n",
    "print(f\"ğŸš€ ìµœì¢… í”¼ì²˜ ê°œìˆ˜: {len(FINAL_COMBINATION)}ê°œ\")\n",
    "print(f\"   ëª©ë¡: {FINAL_COMBINATION}\")\n",
    "\n",
    "# 3. ë°ì´í„°ì…‹ ì¤€ë¹„\n",
    "X_final = X_all[FINAL_COMBINATION].copy()\n",
    "y_final = y\n",
    "\n",
    "# 4. ê²€ì¦ (íŒŒë¼ë¯¸í„°ëŠ” ê¸°ì¡´ ìµœì ê°’ ì‚¬ìš©)\n",
    "# Mayoê°€ ë“¤ì–´ê°”ìœ¼ë‹ˆ colsample_bytreeë§Œ ì‚´ì§ ë‚®ì¶°ì„œ(0.5) Mayo ë…ì¬ë¥¼ ë§‰ì•„ì¤ë‹ˆë‹¤.\n",
    "final_params_xgb = {\n",
    "    'n_estimators': 788, \n",
    "    'learning_rate': 0.06238, \n",
    "    'max_depth': 6, \n",
    "    'min_child_weight': 1, \n",
    "    'gamma': 0.396, \n",
    "    'subsample': 0.921, \n",
    "    'colsample_bytree': 0.5,  # <--- [ìˆ˜ì •] Mayo ê³¼ì í•© ë°©ì§€ìš© (ê¸°ì¡´ 0.549 -> 0.5)\n",
    "    'reg_alpha': 0.0027, \n",
    "    'reg_lambda': 0.0154, \n",
    "    'objective': 'multi:softprob', \n",
    "    'random_state': 42, \n",
    "    'n_jobs': -1,\n",
    "    'verbosity': 0\n",
    "}\n",
    "\n",
    "print(\"\\nğŸš€ [Final Check] 5-Fold CV ì‹œì‘...\")\n",
    "kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "fold_scores = []\n",
    "model = XGBClassifier(**final_params_xgb)\n",
    "\n",
    "for i, (train_idx, val_idx) in enumerate(kf.split(X_final, y_final), 1):\n",
    "    X_tr, X_val = X_final.iloc[train_idx], X_final.iloc[val_idx]\n",
    "    y_tr, y_val = y_final[train_idx], y_final[val_idx]\n",
    "    \n",
    "    model.fit(X_tr, y_tr)\n",
    "    preds = model.predict_proba(X_val)\n",
    "    \n",
    "    if preds.shape[1] > 2:\n",
    "        y_bin = label_binarize(y_val, classes=np.unique(y_final))\n",
    "        score = average_precision_score(y_bin, preds, average=\"macro\")\n",
    "    else:\n",
    "        score = average_precision_score(y_val, preds[:, 1], average=\"macro\")\n",
    "        \n",
    "    fold_scores.append(score)\n",
    "    print(f\"   [Fold {i}] mAP: {score:.5f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(f\"ğŸ† ìµœì¢… í‰ê·  mAP: {np.mean(fold_scores):.6f}\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# 5. ì´ ì ìˆ˜ê°€ 0.699ë³´ë‹¤ ë†’ìœ¼ë©´ ë°”ë¡œ ì´ê±¸ë¡œ ì €ì¥\n",
    "if np.mean(fold_scores) > 0.685: # ì•„ê¹Œ 0.685ë³´ë‹¨ ë¬´ì¡°ê±´ ë†’ì•„ì•¼ í•¨\n",
    "    X_xgb = X_final\n",
    "    X_xgb_test = X_all_test[FINAL_COMBINATION].copy()\n",
    "    print(\"âœ… X_xgb ë°ì´í„°ì…‹ì´ [ì±”í”¼ì–¸+Mayo] ì¡°í•©ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2fa49be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-25 19:48:54,677] A new study created in memory with name: no-name-af1e6cd1-a561-4705-962e-8b989f9f5f23\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ í˜„ì¬ ìµœê³  ê¸°ë¡ì— ë„ì „í•©ë‹ˆë‹¤. (Feature ìˆ˜: 39)\n",
      "\n",
      "ğŸš€ ìƒ˜í”Œë§ ë¹„ìœ¨ ì •ë°€ íŠœë‹ ì‹œì‘ (30íšŒ)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad36af09e356450c8200594c361f3311",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-25 19:49:03,395] Trial 0 finished with value: 0.6951047153226337 and parameters: {'colsample_bytree': 0.7928885003809546, 'subsample': 0.864091722324958}. Best is trial 0 with value: 0.6951047153226337.\n",
      "[I 2025-11-25 19:49:15,420] Trial 1 finished with value: 0.6926670836357881 and parameters: {'colsample_bytree': 0.7436166029602881, 'subsample': 0.6475739139275513}. Best is trial 0 with value: 0.6951047153226337.\n",
      "[I 2025-11-25 19:49:27,599] Trial 2 finished with value: 0.6940191127540823 and parameters: {'colsample_bytree': 0.6358989262830876, 'subsample': 0.751328638433239}. Best is trial 0 with value: 0.6951047153226337.\n",
      "[I 2025-11-25 19:49:43,570] Trial 3 finished with value: 0.6943264068422199 and parameters: {'colsample_bytree': 0.7653239810592734, 'subsample': 0.6933427342615919}. Best is trial 0 with value: 0.6951047153226337.\n",
      "[I 2025-11-25 19:49:58,504] Trial 4 finished with value: 0.7002991388935882 and parameters: {'colsample_bytree': 0.3182957983107913, 'subsample': 0.8017361450253926}. Best is trial 4 with value: 0.7002991388935882.\n",
      "[I 2025-11-25 19:50:12,883] Trial 5 finished with value: 0.7010658901502779 and parameters: {'colsample_bytree': 0.7342715801908728, 'subsample': 0.9759100013865996}. Best is trial 5 with value: 0.7010658901502779.\n",
      "[I 2025-11-25 19:50:26,482] Trial 6 finished with value: 0.698534441808302 and parameters: {'colsample_bytree': 0.647435185434625, 'subsample': 0.9652294623618828}. Best is trial 5 with value: 0.7010658901502779.\n",
      "[I 2025-11-25 19:50:38,741] Trial 7 finished with value: 0.7005057053826843 and parameters: {'colsample_bytree': 0.5480213306633669, 'subsample': 0.8846037801164948}. Best is trial 5 with value: 0.7010658901502779.\n",
      "[I 2025-11-25 19:50:51,988] Trial 8 finished with value: 0.6895629583293272 and parameters: {'colsample_bytree': 0.6261759959233634, 'subsample': 0.6861987062181989}. Best is trial 5 with value: 0.7010658901502779.\n",
      "[I 2025-11-25 19:51:04,621] Trial 9 finished with value: 0.6944790467151982 and parameters: {'colsample_bytree': 0.4458528920984892, 'subsample': 0.610497601925625}. Best is trial 5 with value: 0.7010658901502779.\n",
      "[I 2025-11-25 19:51:15,809] Trial 10 finished with value: 0.6985905965344622 and parameters: {'colsample_bytree': 0.47758436848615177, 'subsample': 0.9746578506217627}. Best is trial 5 with value: 0.7010658901502779.\n",
      "[I 2025-11-25 19:51:27,633] Trial 11 finished with value: 0.7007151267291043 and parameters: {'colsample_bytree': 0.5393190515540531, 'subsample': 0.8821832217049098}. Best is trial 5 with value: 0.7010658901502779.\n",
      "[I 2025-11-25 19:51:40,391] Trial 12 finished with value: 0.7016871630257985 and parameters: {'colsample_bytree': 0.5243025695221816, 'subsample': 0.9177667430125294}. Best is trial 12 with value: 0.7016871630257985.\n",
      "[I 2025-11-25 19:51:51,757] Trial 13 finished with value: 0.7002317526367905 and parameters: {'colsample_bytree': 0.37242086024314325, 'subsample': 0.9918843822902017}. Best is trial 12 with value: 0.7016871630257985.\n",
      "[I 2025-11-25 19:52:07,166] Trial 14 finished with value: 0.7005734538505817 and parameters: {'colsample_bytree': 0.693659370482127, 'subsample': 0.9192907589668201}. Best is trial 12 with value: 0.7016871630257985.\n",
      "[I 2025-11-25 19:52:21,558] Trial 15 finished with value: 0.6979414884056019 and parameters: {'colsample_bytree': 0.46820656176841496, 'subsample': 0.8210606989519259}. Best is trial 12 with value: 0.7016871630257985.\n",
      "[I 2025-11-25 19:52:31,357] Trial 16 finished with value: 0.6973831633438724 and parameters: {'colsample_bytree': 0.6001788245623858, 'subsample': 0.929069659380487}. Best is trial 12 with value: 0.7016871630257985.\n",
      "[I 2025-11-25 19:52:40,375] Trial 17 finished with value: 0.6981702735241955 and parameters: {'colsample_bytree': 0.7203081159719649, 'subsample': 0.9444460961537716}. Best is trial 12 with value: 0.7016871630257985.\n",
      "[I 2025-11-25 19:52:48,102] Trial 18 finished with value: 0.6997025684869334 and parameters: {'colsample_bytree': 0.524719606839786, 'subsample': 0.9999980217392951}. Best is trial 12 with value: 0.7016871630257985.\n",
      "[I 2025-11-25 19:52:58,310] Trial 19 finished with value: 0.6978107629138646 and parameters: {'colsample_bytree': 0.4141012130995262, 'subsample': 0.8454363957017181}. Best is trial 12 with value: 0.7016871630257985.\n",
      "[I 2025-11-25 19:53:11,820] Trial 20 finished with value: 0.695973920759824 and parameters: {'colsample_bytree': 0.5900869789027592, 'subsample': 0.7558465827116965}. Best is trial 12 with value: 0.7016871630257985.\n",
      "[I 2025-11-25 19:53:26,340] Trial 21 finished with value: 0.6966598479463678 and parameters: {'colsample_bytree': 0.5110811470215412, 'subsample': 0.8947789176302614}. Best is trial 12 with value: 0.7016871630257985.\n",
      "[I 2025-11-25 19:53:41,246] Trial 22 finished with value: 0.7000629433760064 and parameters: {'colsample_bytree': 0.5565907931107507, 'subsample': 0.9114243878083379}. Best is trial 12 with value: 0.7016871630257985.\n",
      "[I 2025-11-25 19:53:53,458] Trial 23 finished with value: 0.6990742924948197 and parameters: {'colsample_bytree': 0.5646511246815883, 'subsample': 0.9487870511949597}. Best is trial 12 with value: 0.7016871630257985.\n",
      "[I 2025-11-25 19:54:05,391] Trial 24 finished with value: 0.698668671434267 and parameters: {'colsample_bytree': 0.6771107428445746, 'subsample': 0.859597517214015}. Best is trial 12 with value: 0.7016871630257985.\n",
      "[I 2025-11-25 19:54:15,650] Trial 25 finished with value: 0.6996045206511801 and parameters: {'colsample_bytree': 0.505688259588196, 'subsample': 0.8870482984139817}. Best is trial 12 with value: 0.7016871630257985.\n",
      "[I 2025-11-25 19:54:28,202] Trial 26 finished with value: 0.6953801200628417 and parameters: {'colsample_bytree': 0.598999023236929, 'subsample': 0.8341894414237929}. Best is trial 12 with value: 0.7016871630257985.\n",
      "[I 2025-11-25 19:54:39,873] Trial 27 finished with value: 0.6997095784157977 and parameters: {'colsample_bytree': 0.38728616579141173, 'subsample': 0.9565803158688215}. Best is trial 12 with value: 0.7016871630257985.\n",
      "[I 2025-11-25 19:54:56,642] Trial 28 finished with value: 0.6953802108214209 and parameters: {'colsample_bytree': 0.4420803399666373, 'subsample': 0.7617318503389177}. Best is trial 12 with value: 0.7016871630257985.\n",
      "[I 2025-11-25 19:55:11,646] Trial 29 finished with value: 0.6959554318644707 and parameters: {'colsample_bytree': 0.6772198531652025, 'subsample': 0.8680895155537688}. Best is trial 12 with value: 0.7016871630257985.\n",
      "\n",
      "ğŸ† Best mAP: 0.701687\n",
      "âœ¨ ì°¾ì€ í™©ê¸ˆ ë¹„ìœ¨:\n",
      "   - colsample_bytree (í”¼ì²˜ ëœë¤): 0.5243\n",
      "   - subsample (ë°ì´í„° ëœë¤): 0.9178\n"
     ]
    }
   ],
   "source": [
    "# %% [Fine Tuning] ìƒ˜í”Œë§ ë¹„ìœ¨ ì •ë°€ íŠœë‹ (í™©ê¸ˆ ë¹„ìœ¨ ì°¾ê¸°)\n",
    "\n",
    "import optuna\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import numpy as np\n",
    "\n",
    "# 1. ë°ì´í„° ì¤€ë¹„ (ì•„ê¹Œ ì„±ê³µí•œ X_final ê·¸ëŒ€ë¡œ ì‚¬ìš©)\n",
    "# ë§Œì•½ ë³€ìˆ˜ê°€ ë‚ ì•„ê°”ë‹¤ë©´ ë‹¤ì‹œ ì •ì˜í•˜ì„¸ìš”.\n",
    "# X_final = ... \n",
    "# y = ...\n",
    "\n",
    "print(f\"ğŸ¯ í˜„ì¬ ìµœê³  ê¸°ë¡ì— ë„ì „í•©ë‹ˆë‹¤. (Feature ìˆ˜: {X_final.shape[1]})\")\n",
    "\n",
    "def score_function(y_true, y_pred_proba):\n",
    "    if y_pred_proba.shape[1] > 2:\n",
    "        y_bin = label_binarize(y_true, classes=np.unique(y_true))\n",
    "        return average_precision_score(y_bin, y_pred_proba, average=\"macro\")\n",
    "    else:\n",
    "        return average_precision_score(y_true, y_pred_proba[:, 1], average=\"macro\")\n",
    "\n",
    "def objective_sampling(trial):\n",
    "    # 1. ìƒ˜í”Œë§ íŒŒë¼ë¯¸í„° ì§‘ì¤‘ íƒìƒ‰\n",
    "    # 0.3 (ì—„ê²©í•œ ê²¬ì œ) ~ 0.8 (ì•½í•œ ê²¬ì œ) ì‚¬ì´ë¥¼ ìƒ…ìƒ…ì´ ë’¤ì§‘ë‹ˆë‹¤.\n",
    "    colsamp = trial.suggest_float('colsample_bytree', 0.3, 0.8)\n",
    "    subsamp = trial.suggest_float('subsample', 0.6, 1.0)\n",
    "    \n",
    "    # 2. ë‚˜ë¨¸ì§€ íŒŒë¼ë¯¸í„°ëŠ” 'ì„±ê³µí–ˆë˜ ê°’'ìœ¼ë¡œ ê³ ì • (ë³€ìˆ˜ í†µì œ)\n",
    "    # (ì•„ê¹Œ 0.704 ì°ì—ˆë˜ ê·¸ ì„¤ì •ì…ë‹ˆë‹¤)\n",
    "    param = {\n",
    "        'n_estimators': 1000,\n",
    "        'learning_rate': 0.06238,\n",
    "        'max_depth': 6,\n",
    "        'min_child_weight': 1,\n",
    "        'gamma': 0.396,\n",
    "        'reg_alpha': 0.0027,\n",
    "        'reg_lambda': 0.0154,\n",
    "        \n",
    "        # íŠœë‹ ëŒ€ìƒ\n",
    "        'colsample_bytree': colsamp,\n",
    "        'subsample': subsamp,\n",
    "        \n",
    "        'objective': 'multi:softprob',\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1,\n",
    "        'verbosity': 0\n",
    "    }\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    scores = []\n",
    "    \n",
    "    for train_idx, val_idx in skf.split(X_final, y):\n",
    "        X_tr, X_val = X_final.iloc[train_idx], X_final.iloc[val_idx]\n",
    "        y_tr, y_val = y[train_idx], y[val_idx]\n",
    "        \n",
    "        model = XGBClassifier(**param)\n",
    "        model.fit(X_tr, y_tr)\n",
    "        preds = model.predict_proba(X_val)\n",
    "        scores.append(score_function(y_val, preds))\n",
    "\n",
    "    return np.mean(scores)\n",
    "\n",
    "print(\"\\nğŸš€ ìƒ˜í”Œë§ ë¹„ìœ¨ ì •ë°€ íŠœë‹ ì‹œì‘ (30íšŒ)...\")\n",
    "study_samp = optuna.create_study(direction='maximize')\n",
    "study_samp.optimize(objective_sampling, n_trials=30, show_progress_bar=True)\n",
    "\n",
    "print(f\"\\nğŸ† Best mAP: {study_samp.best_value:.6f}\")\n",
    "print(\"âœ¨ ì°¾ì€ í™©ê¸ˆ ë¹„ìœ¨:\")\n",
    "print(f\"   - colsample_bytree (í”¼ì²˜ ëœë¤): {study_samp.best_params['colsample_bytree']:.4f}\")\n",
    "print(f\"   - subsample (ë°ì´í„° ëœë¤): {study_samp.best_params['subsample']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1a412eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-26 06:40:49,163] A new study created in memory with name: no-name-83a86386-1a3d-4a06-a125-f1943390e38e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ LGBM ìƒ˜í”Œë§ ìµœì í™” ì‹œì‘ (Base Feature ìˆ˜: 22)\n",
      "\n",
      "ğŸš€ LGBM ìƒ˜í”Œë§ ë¹„ìœ¨ íŠœë‹ ì‹œì‘ (30íšŒ)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e18658ee69240708cb0f6b6cab243aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-26 06:41:12,535] Trial 0 finished with value: 0.6941275694802227 and parameters: {'colsample_bytree': 0.6848904570319175, 'subsample': 0.5232583014550416}. Best is trial 0 with value: 0.6941275694802227.\n",
      "[I 2025-11-26 06:41:41,508] Trial 1 finished with value: 0.6966609679283466 and parameters: {'colsample_bytree': 0.5915280229860977, 'subsample': 0.9267769135624113}. Best is trial 1 with value: 0.6966609679283466.\n",
      "[I 2025-11-26 06:42:10,999] Trial 2 finished with value: 0.6967511511723345 and parameters: {'colsample_bytree': 0.745574712812131, 'subsample': 0.7560804742359675}. Best is trial 2 with value: 0.6967511511723345.\n",
      "[I 2025-11-26 06:42:30,277] Trial 3 finished with value: 0.6979561380544173 and parameters: {'colsample_bytree': 0.7573334514813482, 'subsample': 0.5721253088710312}. Best is trial 3 with value: 0.6979561380544173.\n",
      "[I 2025-11-26 06:42:51,828] Trial 4 finished with value: 0.69893184684255 and parameters: {'colsample_bytree': 0.34640559220178857, 'subsample': 0.7288621760043279}. Best is trial 4 with value: 0.69893184684255.\n",
      "[I 2025-11-26 06:43:15,228] Trial 5 finished with value: 0.696259592004865 and parameters: {'colsample_bytree': 0.7010368634150136, 'subsample': 0.7965861772244479}. Best is trial 4 with value: 0.69893184684255.\n",
      "[I 2025-11-26 06:43:39,031] Trial 6 finished with value: 0.6953084664264212 and parameters: {'colsample_bytree': 0.681409580554899, 'subsample': 0.8133914520792931}. Best is trial 4 with value: 0.69893184684255.\n",
      "[I 2025-11-26 06:43:59,319] Trial 7 finished with value: 0.6997921276064822 and parameters: {'colsample_bytree': 0.4053253867683405, 'subsample': 0.6066679579912073}. Best is trial 7 with value: 0.6997921276064822.\n",
      "[I 2025-11-26 06:44:18,271] Trial 8 finished with value: 0.6949006218325795 and parameters: {'colsample_bytree': 0.6158946060528351, 'subsample': 0.5075922949882121}. Best is trial 7 with value: 0.6997921276064822.\n",
      "[I 2025-11-26 06:44:50,879] Trial 9 finished with value: 0.6961854311054803 and parameters: {'colsample_bytree': 0.6845807530839441, 'subsample': 0.6364121040873612}. Best is trial 7 with value: 0.6997921276064822.\n",
      "[I 2025-11-26 06:45:10,988] Trial 10 finished with value: 0.6982102572163414 and parameters: {'colsample_bytree': 0.42898114104742485, 'subsample': 0.674564823561015}. Best is trial 7 with value: 0.6997921276064822.\n",
      "[I 2025-11-26 06:45:29,347] Trial 11 finished with value: 0.6995112309831152 and parameters: {'colsample_bytree': 0.30059185900448016, 'subsample': 0.6808686866247351}. Best is trial 7 with value: 0.6997921276064822.\n",
      "[I 2025-11-26 06:45:46,701] Trial 12 finished with value: 0.6995734816252531 and parameters: {'colsample_bytree': 0.3065211305189013, 'subsample': 0.6327944790309163}. Best is trial 7 with value: 0.6997921276064822.\n",
      "[I 2025-11-26 06:46:06,986] Trial 13 finished with value: 0.697016952909016 and parameters: {'colsample_bytree': 0.44565427350734543, 'subsample': 0.5986378792144835}. Best is trial 7 with value: 0.6997921276064822.\n",
      "[I 2025-11-26 06:46:25,242] Trial 14 finished with value: 0.6987992379853879 and parameters: {'colsample_bytree': 0.44010776464360624, 'subsample': 0.581538422915907}. Best is trial 7 with value: 0.6997921276064822.\n",
      "[I 2025-11-26 06:46:49,288] Trial 15 finished with value: 0.6983333882598821 and parameters: {'colsample_bytree': 0.3734390066144397, 'subsample': 0.8819219707343822}. Best is trial 7 with value: 0.6997921276064822.\n",
      "[I 2025-11-26 06:47:09,296] Trial 16 finished with value: 0.6975493401886479 and parameters: {'colsample_bytree': 0.4988836794007879, 'subsample': 0.6606226426518484}. Best is trial 7 with value: 0.6997921276064822.\n",
      "[I 2025-11-26 06:47:31,751] Trial 17 finished with value: 0.6991257962713517 and parameters: {'colsample_bytree': 0.36922658498799654, 'subsample': 0.7237803598720565}. Best is trial 7 with value: 0.6997921276064822.\n",
      "[I 2025-11-26 06:47:49,417] Trial 18 finished with value: 0.6955609391658779 and parameters: {'colsample_bytree': 0.5299903285025418, 'subsample': 0.5482485341754598}. Best is trial 7 with value: 0.6997921276064822.\n",
      "[I 2025-11-26 06:48:06,932] Trial 19 finished with value: 0.6987941737585991 and parameters: {'colsample_bytree': 0.3032065798460728, 'subsample': 0.6205909406180847}. Best is trial 7 with value: 0.6997921276064822.\n",
      "[I 2025-11-26 06:48:30,934] Trial 20 finished with value: 0.6994846278929456 and parameters: {'colsample_bytree': 0.405799839810132, 'subsample': 0.8442643254818438}. Best is trial 7 with value: 0.6997921276064822.\n",
      "[I 2025-11-26 06:48:49,824] Trial 21 finished with value: 0.7002174250580132 and parameters: {'colsample_bytree': 0.3038891644219964, 'subsample': 0.6888722481028347}. Best is trial 21 with value: 0.7002174250580132.\n",
      "[I 2025-11-26 06:49:13,324] Trial 22 finished with value: 0.7015100242028046 and parameters: {'colsample_bytree': 0.33562465934865593, 'subsample': 0.6965417562200007}. Best is trial 22 with value: 0.7015100242028046.\n",
      "[I 2025-11-26 06:49:53,897] Trial 23 finished with value: 0.696691189179713 and parameters: {'colsample_bytree': 0.49378187484929453, 'subsample': 0.9943623075066883}. Best is trial 22 with value: 0.7015100242028046.\n",
      "[I 2025-11-26 06:50:26,064] Trial 24 finished with value: 0.6996179298896329 and parameters: {'colsample_bytree': 0.357739641815848, 'subsample': 0.7079758737648351}. Best is trial 22 with value: 0.7015100242028046.\n",
      "[I 2025-11-26 06:52:02,434] Trial 25 finished with value: 0.6969386537326483 and parameters: {'colsample_bytree': 0.3978742669096411, 'subsample': 0.7515902233795038}. Best is trial 22 with value: 0.7015100242028046.\n",
      "[I 2025-11-26 06:53:43,252] Trial 26 finished with value: 0.6992299465226848 and parameters: {'colsample_bytree': 0.343408343436256, 'subsample': 0.6951575149095091}. Best is trial 22 with value: 0.7015100242028046.\n",
      "[I 2025-11-26 06:55:16,727] Trial 27 finished with value: 0.6983994857403459 and parameters: {'colsample_bytree': 0.4760775037630187, 'subsample': 0.7852876174485827}. Best is trial 22 with value: 0.7015100242028046.\n",
      "[I 2025-11-26 06:56:31,190] Trial 28 finished with value: 0.7011475726059839 and parameters: {'colsample_bytree': 0.3273594315531516, 'subsample': 0.6528922185384414}. Best is trial 22 with value: 0.7015100242028046.\n",
      "[I 2025-11-26 06:57:10,238] Trial 29 finished with value: 0.7010388103504398 and parameters: {'colsample_bytree': 0.33566362324675836, 'subsample': 0.6599649101165308}. Best is trial 22 with value: 0.7015100242028046.\n",
      "\n",
      "ğŸ† Best LGBM mAP: 0.701510\n",
      "âœ¨ ì°¾ì€ í™©ê¸ˆ ë¹„ìœ¨:\n",
      "   - colsample_bytree: 0.3356\n",
      "   - subsample: 0.6965\n",
      "\n",
      "âœ… ìµœì¢… íŒŒë¼ë¯¸í„° ì¤€ë¹„ ì™„ë£Œ (final_params_lgb_tuned)\n"
     ]
    }
   ],
   "source": [
    "# %% [LGBM Fine Tuning] ê¸°ì¡´ íŒŒë¼ë¯¸í„° ê³ ì • + ìƒ˜í”Œë§ ë¹„ìœ¨ë§Œ ì§‘ì¤‘ íŠœë‹\n",
    "\n",
    "import optuna\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import numpy as np\n",
    "\n",
    "# 1. ê¸°ì¡´ì— ì°¾ì€ ê¿€ íŒŒë¼ë¯¸í„° (ê³ ì •ê°’)\n",
    "base_params_lgb = {\n",
    "    'n_estimators': 912,\n",
    "    'learning_rate': 0.01090933368427726,\n",
    "    'num_leaves': 193,\n",
    "    'max_depth': 14,\n",
    "    'min_child_samples': 51,\n",
    "    # 'subsample': 0.5933...  <-- ì´ê±´ íŠœë‹í•  ê±°ë‹ˆê¹Œ ì œì™¸\n",
    "    # 'colsample_bytree': 0.5036... <-- ì´ê²ƒë„ íŠœë‹ ëŒ€ìƒ\n",
    "    'reg_alpha': 0.008835424632453443,\n",
    "    'reg_lambda': 0.07034822683895961,\n",
    "    'objective': 'multiclass',\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1,\n",
    "    'verbosity': -1\n",
    "}\n",
    "\n",
    "# 2. ë°ì´í„° ì¤€ë¹„ (Mayoê°€ í¬í•¨ëœ X_lgb_final ì‚¬ìš©)\n",
    "# X_lgb_final = ... (ì´ì „ ë‹¨ê³„ì—ì„œ ë§Œë“¤ì–´ì§„ ë°ì´í„°ì…‹)\n",
    "# y = ...\n",
    "\n",
    "print(f\"ğŸ¯ LGBM ìƒ˜í”Œë§ ìµœì í™” ì‹œì‘ (Base Feature ìˆ˜: {X_lgb_final.shape[1]})\")\n",
    "\n",
    "def score_function(y_true, y_pred_proba):\n",
    "    if y_pred_proba.shape[1] > 2:\n",
    "        y_bin = label_binarize(y_true, classes=np.unique(y_true))\n",
    "        return average_precision_score(y_bin, y_pred_proba, average=\"macro\")\n",
    "    else:\n",
    "        return average_precision_score(y_true, y_pred_proba[:, 1], average=\"macro\")\n",
    "\n",
    "def objective_lgb_sampling(trial):\n",
    "    # íŠœë‹ ëŒ€ìƒ: ìƒ˜í”Œë§ íŒŒë¼ë¯¸í„°ë§Œ (0.3 ~ 0.8 ë²”ìœ„ ì§‘ì¤‘ íƒìƒ‰)\n",
    "    # Mayo Scoreê°€ ê°•ë ¥í•˜ë¯€ë¡œ, í”¼ì²˜ë¥¼ ì ê²Œ ë½‘ëŠ”(0.3~0.5) ìª½ì´ ìœ ë¦¬í•  ìˆ˜ ìˆìŒ\n",
    "    colsamp = trial.suggest_float('colsample_bytree', 0.3, 0.8)\n",
    "    subsamp = trial.suggest_float('subsample', 0.5, 1.0)\n",
    "    \n",
    "    # íŒŒë¼ë¯¸í„° í•©ì¹˜ê¸°\n",
    "    param = base_params_lgb.copy()\n",
    "    param['colsample_bytree'] = colsamp\n",
    "    param['subsample'] = subsamp\n",
    "    param['subsample_freq'] = 1 # ë°°ê¹… í™œì„±í™”ë¥¼ ìœ„í•´ í•„ìˆ˜\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    scores = []\n",
    "    \n",
    "    for train_idx, val_idx in skf.split(X_lgb_final, y):\n",
    "        X_tr, X_val = X_lgb_final.iloc[train_idx], X_lgb_final.iloc[val_idx]\n",
    "        y_tr, y_val = y[train_idx], y[val_idx]\n",
    "        \n",
    "        model = LGBMClassifier(**param)\n",
    "        model.fit(X_tr, y_tr)\n",
    "        preds = model.predict_proba(X_val)\n",
    "        scores.append(score_function(y_val, preds))\n",
    "\n",
    "    return np.mean(scores)\n",
    "\n",
    "print(\"\\nğŸš€ LGBM ìƒ˜í”Œë§ ë¹„ìœ¨ íŠœë‹ ì‹œì‘ (30íšŒ)...\")\n",
    "study_lgb_samp = optuna.create_study(direction='maximize')\n",
    "study_lgb_samp.optimize(objective_lgb_sampling, n_trials=30, show_progress_bar=True)\n",
    "\n",
    "print(f\"\\nğŸ† Best LGBM mAP: {study_lgb_samp.best_value:.6f}\")\n",
    "print(\"âœ¨ ì°¾ì€ í™©ê¸ˆ ë¹„ìœ¨:\")\n",
    "print(f\"   - colsample_bytree: {study_lgb_samp.best_params['colsample_bytree']:.4f}\")\n",
    "print(f\"   - subsample: {study_lgb_samp.best_params['subsample']:.4f}\")\n",
    "\n",
    "# 3. ìµœì¢… íŒŒë¼ë¯¸í„° í™•ì •\n",
    "final_params_lgb_tuned = base_params_lgb.copy()\n",
    "final_params_lgb_tuned.update(study_lgb_samp.best_params)\n",
    "final_params_lgb_tuned['subsample_freq'] = 1\n",
    "\n",
    "print(\"\\nâœ… ìµœì¢… íŒŒë¼ë¯¸í„° ì¤€ë¹„ ì™„ë£Œ (final_params_lgb_tuned)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
