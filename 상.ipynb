{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f692faa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (6500, 24)\n",
      "Test shape:  (1405, 23)\n",
      "Preprocessing Start...\n",
      "Preprocessing Done.\n",
      "Applying Smart Scaling on 105 columns...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, QuantileTransformer, label_binarize\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.base import clone\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from scipy.stats import rankdata\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 1. ì„¤ì • ë° ë°ì´í„° ë¡œë“œ\n",
    "# ----------------------------------------------------\n",
    "RANDOM_STATE = 42\n",
    "N_SPLITS = 10 \n",
    "\n",
    "train_df = pd.read_csv(r\"C:\\Users\\abc01\\OneDrive\\ë°”íƒ• í™”ë©´\\train.csv\")\n",
    "test_df  = pd.read_csv(r\"C:\\Users\\abc01\\OneDrive\\ë°”íƒ• í™”ë©´\\test.csv\")\n",
    "\n",
    "print(f\"Train shape: {train_df.shape}\")\n",
    "print(f\"Test shape:  {test_df.shape}\")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 2. ì „ì²˜ë¦¬ í•¨ìˆ˜ ì •ì˜ (ëˆ„ìˆ˜ ë°©ì§€ & ìµœì í™” ë²„ì „)\n",
    "# ----------------------------------------------------\n",
    "\n",
    "def prepare_raw_simple(df):\n",
    "    df = df.copy()\n",
    "    if \"birth_date\" in df.columns:\n",
    "        df[\"birth_date\"] = pd.to_datetime(df[\"birth_date\"])\n",
    "        df[\"age\"] = 2025 - df[\"birth_date\"].dt.year\n",
    "    if \"gender\" in df.columns:\n",
    "        df[\"gender_int\"] = df[\"gender\"].map({\"M\": 0, \"F\": 1})\n",
    "    return df\n",
    "\n",
    "def add_eda_rule_features_v2(df):\n",
    "    df = df.copy()\n",
    "    # chem_01_range\n",
    "    df['chem_01_range_str'] = pd.cut(\n",
    "        df['chem_01'], bins=[-np.inf, 2.0, 4.0, np.inf], \n",
    "        labels=['Normal_Low', 'Warning_Zone', 'Danger_High']\n",
    "    ).astype(str)\n",
    "    chem_map = {'Normal_Low': 0, 'Warning_Zone': 1, 'Danger_High': 2}\n",
    "    df['chem_01_range'] = df['chem_01_range_str'].map(chem_map).astype(int)\n",
    "\n",
    "    # trace_metal_range\n",
    "    df['trace_metal_range_str'] = pd.cut(\n",
    "        df['trace_metal'], bins=[-np.inf, 90, 130, np.inf],\n",
    "        labels=['Safe', 'Caution', 'Risk']\n",
    "    ).astype(str)\n",
    "    metal_map = {'Safe': 0, 'Caution': 1, 'Risk': 2}\n",
    "    df['trace_metal_range'] = df['trace_metal_range_str'].map(metal_map).astype(int)\n",
    "\n",
    "    # risk_segment\n",
    "    def check_ambiguous_risk(row):\n",
    "        if (row['swelling'] == 'S') and (2.0 <= row['chem_01'] < 5.0):\n",
    "            return 'Target_1_Suspect'\n",
    "        elif (row['swelling'] == 'Y') or (row['chem_01'] >= 5.0):\n",
    "            return 'High_Risk'\n",
    "        else:\n",
    "            return 'Normal'\n",
    "    df['risk_segment_str'] = df.apply(check_ambiguous_risk, axis=1).astype(str)\n",
    "    risk_map = {'Normal': 0, 'Target_1_Suspect': 1, 'High_Risk': 2}\n",
    "    df['risk_segment'] = df['risk_segment_str'].map(risk_map).astype(int)\n",
    "    return df\n",
    "\n",
    "def make_boosting_features_v2(df):\n",
    "    df = df.copy()\n",
    "    high_is_healthy = [\"protein_level\", \"blood_cells\"]\n",
    "    low_is_healthy  = [\"enzyme_A\", \"enzyme_B\", \"lipid_index\", \"trace_metal\", \"clot_time\"]\n",
    "    def minmax(s):\n",
    "        s = s.astype(float)\n",
    "        mn, mx = s.min(), s.max()\n",
    "        if mx == mn: return pd.Series(0.5, index=s.index)\n",
    "        return (s - mn) / (mx - mn + 1e-6)\n",
    "\n",
    "    if all(col in df.columns for col in high_is_healthy + low_is_healthy):\n",
    "        df[\"health_index\"] = (sum(minmax(df[col]) for col in high_is_healthy) - sum(minmax(df[col]) for col in low_is_healthy))\n",
    "        df[\"health_per_stage\"] = df[\"health_index\"] / (df[\"disease_stage\"] + 1e-6)\n",
    "\n",
    "    df[\"chem01_trace_combo\"] = df[\"chem_01\"] * df[\"trace_metal\"]\n",
    "    df[\"chem01_chem02_combo\"] = df[\"chem_01\"] * df[\"chem_02\"]\n",
    "    df[\"chem01_enzymeB_combo\"] = df[\"chem_01\"] * df[\"enzyme_B\"]\n",
    "    df[\"enzyme_ratio\"] = df[\"enzyme_A\"] / (df[\"enzyme_B\"].replace(0, np.nan) + 1e-6)\n",
    "    df[\"lipid_blood_ratio\"] = df[\"lipid_index\"] / (df[\"blood_cells\"].replace(0, np.nan) + 1e-6)\n",
    "    df[\"chem02_trace_ratio\"] = df[\"chem_02\"] / (df[\"trace_metal\"].replace(0, np.nan) + 1e-6)\n",
    "    df[\"sum_chem\"] = df[\"chem_01\"] + df[\"chem_02\"]\n",
    "    df[\"diff_chem\"] = df[\"chem_01\"] - df[\"chem_02\"]\n",
    "    df[\"sum_enzyme\"] = df[\"enzyme_A\"] + df[\"enzyme_B\"]\n",
    "    df[\"diff_enzyme\"] = df[\"enzyme_A\"] - df[\"enzyme_B\"]\n",
    "\n",
    "    if \"age\" in df.columns:\n",
    "        df[\"obs_per_age\"] = df[\"obs_days\"] / (df[\"age\"] + 1e-6)\n",
    "        df[\"behavior_per_age\"] = df[\"behavior_index\"] / (df[\"age\"] + 1e-6)\n",
    "    df[\"disease_velocity\"] = df[\"disease_stage\"] / (df[\"obs_days\"] + 1e-6)\n",
    "\n",
    "    symptom_cols = [\"fluid_accum\", \"organ_enlarge\", \"vascular_marks\"]\n",
    "    temp_symptoms = df[symptom_cols].replace({\"Y\": 1, \"N\": 0}).infer_objects(copy=False)\n",
    "    df[\"symptom_count\"] = temp_symptoms.sum(axis=1)\n",
    "    if \"swelling\" in df.columns:\n",
    "        df[\"swelling_ord\"] = df[\"swelling\"].map({\"N\": 0, \"S\": 1, \"Y\": 2})\n",
    "    else:\n",
    "        df[\"swelling_ord\"] = 0\n",
    "    df[\"symptom_severity\"] = df[\"symptom_count\"] + df[\"swelling_ord\"]\n",
    "    return df\n",
    "\n",
    "def add_discrete_caution_features(X):\n",
    "    X = X.copy()\n",
    "    X[\"stage_mid\"] = X[\"disease_stage\"].between(2, 3).astype(int)\n",
    "    X[\"beh_mid\"] = (X[\"behavior_index\"] == 1).astype(int)\n",
    "    X[\"caution_symptom_zone\"] = (X[\"symptom_count\"].between(1, 2) & (X[\"swelling_ord\"] >= 1)).astype(int)\n",
    "    X[\"caution_stage_beh\"] = ((X[\"stage_mid\"] == 1) & (X[\"beh_mid\"] == 1)).astype(int)\n",
    "    if X['organ_enlarge'].dtype == 'object':\n",
    "        organ_flag = (X['organ_enlarge'] == 'Y')\n",
    "    else:\n",
    "        organ_flag = (X['organ_enlarge'] == 1)\n",
    "    X[\"caution_organ_only\"] = (organ_flag & (X[\"disease_stage\"] == 1) & (X[\"behavior_index\"] <= 1)).astype(int)\n",
    "    return X\n",
    "\n",
    "def add_post_eda_features(df):\n",
    "    df = df.copy()\n",
    "    df['protein_treatment_interaction'] = df['protein_level'] * (1 + df['treatment'])\n",
    "    df['blood_treatment_interaction']   = df['blood_cells'] * (1 + df['treatment'])\n",
    "    df['chem01_treatment_interaction']  = df['chem_01'] * (1 + df['treatment'])\n",
    "    df['trace_treatment_interaction']   = df['trace_metal'] * (1 + df['treatment'])\n",
    "    df['coagulation_index'] = df['protein_level'] / (df['clot_time'] + 1e-6)\n",
    "    df['coagulation_product'] = df['protein_level'] * df['clot_time']\n",
    "    if df['organ_enlarge'].dtype == 'object':\n",
    "        organ_val = (df['organ_enlarge'] == 'Y').astype(int)\n",
    "    else:\n",
    "        organ_val = df['organ_enlarge']\n",
    "    df['deadly_combo'] = df['swelling_ord'] * organ_val * 2\n",
    "    df['log_obs_days'] = np.log1p(df['obs_days'])\n",
    "    df['obs_days_squared'] = df['obs_days'] ** 2\n",
    "    return df\n",
    "\n",
    "# %% [Section 2-2] (ì¶”ê°€) ML_Health ê¸°ë°˜ ê³ ì„±ëŠ¥ íŒŒìƒ í”¼ì²˜ ì´ì‹\n",
    "\n",
    "def add_ml_health_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    \n",
    "    # 1. [í•µì‹¬] í–‰ë™ ì§€ìˆ˜ ëŒ€ë¹„ ë©´ì—­ë ¥ (ì¤‘ìš”ë„ Top 4)\n",
    "    # behavior_indexê°€ 0ì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ 1ì„ ë”í•´ ë‚˜ëˆ”\n",
    "    df['immune_per_behavior'] = df['immune_index'] / (df['behavior_index'] + 1.0)\n",
    "    \n",
    "    # 2. í˜ˆêµ¬ ìˆ˜ ëŒ€ë¹„ ì‘ê³  ì‹œê°„ (ì„¸í¬ ë°€ë„ì™€ ì‘ê³  ê´€ê³„)\n",
    "    df['clot_per_cell'] = df['clot_time'] / (df['blood_cells'] + 1e-6)\n",
    "    \n",
    "    # 3. ë‹¨ë°±ì§ˆ ëŒ€ë¹„ ì§€ì§ˆ ë¹„ìœ¨ (ì˜ì–‘ ë¶ˆê· í˜•)\n",
    "    df['lipid_per_protein'] = df['lipid_index'] / (df['protein_level'] + 1e-6)\n",
    "    \n",
    "    # 4. ì§€ì§ˆê³¼ í™”í•™ìˆ˜ì¹˜ ì°¨ì´\n",
    "    df['lipid_delta'] = df['lipid_index'] - df['chem_01']\n",
    "    \n",
    "    # 5. ê´€ì°° ê¸°ê°„ ëŒ€ë¹„ ë¯¸ë„¤ë„ ë°€ë„\n",
    "    df['trace_density'] = df['trace_metal'] / (df['obs_days'] + 1.0)\n",
    "    \n",
    "    # 6. í–‰ë™ ì§€ìˆ˜ì™€ ì¹˜ë£Œì˜ ìƒí˜¸ì‘ìš©\n",
    "    df['behavior_treatment'] = df['behavior_index'] * (1 + df['treatment'])\n",
    "\n",
    "    return df\n",
    "\n",
    "def add_missing_health_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    FFFFFFINAL_ENS.ipynbì— ì—†ëŠ” ML_Health ê¸°ë°˜ í•µì‹¬ í”¼ì²˜ ì¶”ê°€\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # 1. [í•µì‹¬] í–‰ë™ ì§€ìˆ˜ ëŒ€ë¹„ ë©´ì—­ë ¥ (ML_Health ì¤‘ìš”ë„ Top 4)\n",
    "    # behavior_indexê°€ ë‚®ì„ìˆ˜ë¡(0), ë©´ì—­ë ¥ì´ ì§ˆë³‘ ë°©ì–´ì— ë” ì¤‘ìš”í•  ìˆ˜ ìˆìŒ\n",
    "    df['immune_per_behavior'] = df['immune_index'] / (df['behavior_index'] + 1.0)\n",
    "    \n",
    "    # 2. í˜ˆêµ¬ ìˆ˜ ëŒ€ë¹„ ì‘ê³  ì‹œê°„\n",
    "    # í˜ˆì•¡ ë‚´ ì„¸í¬ ë°€ë„ì™€ ì‘ê³  ëŠ¥ë ¥ì˜ ê´€ê³„\n",
    "    df['clot_per_cell'] = df['clot_time'] / (df['blood_cells'] + 1e-6)\n",
    "    \n",
    "    # 3. ë‹¨ë°±ì§ˆ ëŒ€ë¹„ ì§€ì§ˆ ë¹„ìœ¨ (ì˜ì–‘ ë¶ˆê· í˜• ì§€í‘œ)\n",
    "    df['lipid_per_protein'] = df['lipid_index'] / (df['protein_level'] + 1e-6)\n",
    "    \n",
    "    # 4. ì§€ì§ˆê³¼ í™”í•™ìˆ˜ì¹˜(chem_01) ì°¨ì´\n",
    "    # ì„œë¡œ ë‹¤ë¥¸ ìƒì²´ ì§€í‘œ ê°„ì˜ ê´´ë¦¬ ì •ë„\n",
    "    df['lipid_delta'] = df['lipid_index'] - df['chem_01']\n",
    "    \n",
    "    # 5. ê´€ì°° ê¸°ê°„ ëŒ€ë¹„ ë¯¸ë„¤ë„(Trace Metal) ë°€ë„\n",
    "    # ê´€ì°° ê¸°ê°„ ë™ì•ˆ ë¯¸ë„¤ë„ ìˆ˜ì¹˜ê°€ ì–¼ë§ˆë‚˜ ë†ì¶•ë˜ì—ˆëŠ”ì§€ (ì‹œê°„ ë³´ì •)\n",
    "    df['trace_density'] = df['trace_metal'] / (df['obs_days'] + 1.0)\n",
    "\n",
    "    # 6. í–‰ë™ ì§€ìˆ˜ì™€ ì¹˜ë£Œì˜ ìƒí˜¸ì‘ìš©\n",
    "    # ì¹˜ë£Œë¥¼ ë°›ëŠ” ì‚¬ëŒì˜ í–‰ë™ íŒ¨í„´ ê°€ì¤‘ì¹˜\n",
    "    df['behavior_treatment'] = df['behavior_index'] * (1 + df['treatment'])\n",
    "\n",
    "    return df\n",
    "\n",
    "def add_group_stats_no_leakage(train, test):\n",
    "    train = train.copy()\n",
    "    test = test.copy()\n",
    "    bins = [60, 80, 90, 100, 130]\n",
    "    labels = [\"60-79\", \"80-89\", \"90-99\", \"100+\"]\n",
    "    for df in [train, test]:\n",
    "        df[\"age_group\"] = pd.cut(df[\"age\"], bins=bins, labels=labels, right=False, include_lowest=True)\n",
    "\n",
    "    group_cols = ['disease_stage', 'gender', 'age_group']\n",
    "    agg_cols = ['enzyme_A', 'enzyme_B', 'protein_level', 'immune_index', 'lipid_index', 'blood_cells']\n",
    "\n",
    "    for group_col in group_cols:\n",
    "        for col in agg_cols:\n",
    "            train_groupby = train.groupby(group_col)[col]\n",
    "            mean_map = train_groupby.mean()\n",
    "            std_map  = train_groupby.std()\n",
    "            prefix = f'{col}_by_{group_col}'\n",
    "            for df_target in [train, test]:\n",
    "                mapped_mean = df_target[group_col].map(mean_map).astype(float).values\n",
    "                mapped_std  = df_target[group_col].map(std_map).astype(float).values\n",
    "                col_values  = df_target[col].astype(float).values\n",
    "                df_target[f'{prefix}_diff_mean']  = col_values - mapped_mean\n",
    "                df_target[f'{prefix}_zscore']     = (col_values - mapped_mean) / (mapped_std + 1e-6)\n",
    "                df_target[f'{prefix}_ratio_mean'] = col_values / (mapped_mean + 1e-6)\n",
    "    return train, test\n",
    "\n",
    "def add_qbins_no_leakage(train, test, n_bins=4):\n",
    "    train = train.copy()\n",
    "    test = test.copy()\n",
    "    lab_cols = [\"chem_01\", \"chem_02\", \"protein_level\", \"trace_metal\", \"enzyme_A\", \"enzyme_B\", \"lipid_index\", \"blood_cells\", \"clot_time\"]\n",
    "    for col in lab_cols:\n",
    "        if col not in train.columns: continue\n",
    "        try:\n",
    "            _, bins = pd.qcut(train[col], q=n_bins, retbins=True, duplicates='drop')\n",
    "        except ValueError:\n",
    "            continue\n",
    "        train[f\"{col}_qbin\"] = pd.cut(train[col], bins=bins, labels=False, include_lowest=True).fillna(-1).astype(int)\n",
    "        test[f\"{col}_qbin\"] = pd.cut(test[col], bins=bins, labels=False, include_lowest=True).fillna(-1).astype(int)\n",
    "    return train, test\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 3. ì „ì²˜ë¦¬ ì‹¤í–‰\n",
    "# ----------------------------------------------------\n",
    "print(\"Preprocessing Start...\")\n",
    "train_df = prepare_raw_simple(train_df)\n",
    "test_df = prepare_raw_simple(test_df)\n",
    "\n",
    "train_df = add_eda_rule_features_v2(train_df)\n",
    "test_df  = add_eda_rule_features_v2(test_df)\n",
    "\n",
    "train_df = make_boosting_features_v2(train_df)\n",
    "test_df  = make_boosting_features_v2(test_df)\n",
    "\n",
    "train_df = add_discrete_caution_features(train_df)\n",
    "test_df  = add_discrete_caution_features(test_df)\n",
    "\n",
    "train_df = add_post_eda_features(train_df)\n",
    "test_df  = add_post_eda_features(test_df)\n",
    "\n",
    "train_df = add_missing_health_features(train_df)\n",
    "test_df  = add_missing_health_features(test_df)\n",
    "\n",
    "train_df = add_ml_health_features(train_df)\n",
    "test_df  = add_ml_health_features(test_df)\n",
    "\n",
    "train_df, test_df = add_group_stats_no_leakage(train_df, test_df)\n",
    "train_df, test_df = add_qbins_no_leakage(train_df, test_df)\n",
    "\n",
    "drop_cols = [\"index\", \"name\", \"birth_date\", \"geo_code\", \"birth_date\"]\n",
    "y = train_df[\"target\"].values\n",
    "X_all = train_df.drop(columns=drop_cols + [\"target\"], errors='ignore')\n",
    "X_all_test = test_df.drop(columns=drop_cols, errors='ignore')\n",
    "\n",
    "print(\"Preprocessing Done.\")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 4. ìŠ¤ë§ˆíŠ¸ ìŠ¤ì¼€ì¼ë§ (Skewness ê¸°ë°˜)\n",
    "# ----------------------------------------------------\n",
    "numeric_candidates = X_all.select_dtypes(include=['number']).columns\n",
    "EXCLUDE_COLS = [\n",
    "    \"gender_int\", \"fluid_accum\", \"organ_enlarge\", \"vascular_marks\", \n",
    "    \"swelling\", \"treatment\", \"geo_code\", \"risk_segment\",\n",
    "    \"chem_01_range\", \"trace_metal_range\", \"swelling_ord\",\n",
    "    \"target\", \"index\"\n",
    "]\n",
    "NUMERIC_COLS = [c for c in numeric_candidates if c not in EXCLUDE_COLS and not c.endswith('_qbin')]\n",
    "\n",
    "def apply_skew_scaling(train_df, test_df, cols):\n",
    "    train_df = train_df.copy()\n",
    "    test_df = test_df.copy()\n",
    "    skewness = train_df[cols].skew()\n",
    "    print(f\"Applying Smart Scaling on {len(cols)} columns...\")\n",
    "    \n",
    "    for col in cols:\n",
    "        skew = abs(skewness[col])\n",
    "        if skew < 0.5:\n",
    "            scaler = StandardScaler()\n",
    "        elif skew < 1.5:\n",
    "            scaler = RobustScaler()\n",
    "        else:\n",
    "            scaler = QuantileTransformer(output_distribution=\"normal\", random_state=RANDOM_STATE)\n",
    "        train_df[col] = scaler.fit_transform(train_df[[col]]).flatten()\n",
    "        test_df[col] = scaler.transform(test_df[[col]]).flatten()\n",
    "    return train_df, test_df\n",
    "\n",
    "X_all, X_all_test = apply_skew_scaling(X_all, X_all_test, NUMERIC_COLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bfea7301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------\n",
    "# 5. ìµœì  í”¼ì²˜ ë° í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •\n",
    "# ----------------------------------------------------\n",
    "\n",
    "# [XGBoost]\n",
    "XGB_FEATS_SELECTED = [\n",
    "    'chem_01_range', 'chem01_trace_combo', 'chem_01', 'symptom_count', 'clot_time_qbin', \n",
    "    'symptom_severity', 'health_per_stage', 'obs_days', 'swelling_ord', 'gender_int', \n",
    "    'age', 'lipid_index_by_gender_ratio_mean', 'chem_01_qbin', 'enzyme_B_by_disease_stage_zscore', \n",
    "    'blood_cells_by_gender_zscore', 'enzyme_A_by_gender_diff_mean', 'chem01_treatment_interaction', \n",
    "    'deadly_combo', 'enzyme_A_by_disease_stage_ratio_mean', 'enzyme_B', 'sum_enzyme', \n",
    "    'chem01_chem02_combo', 'sum_chem', 'blood_cells_by_gender_ratio_mean', \n",
    "    'trace_treatment_interaction', 'enzyme_B_by_age_group_ratio_mean', \n",
    "    'lipid_index_by_disease_stage_ratio_mean', 'blood_cells_by_age_group_diff_mean', \n",
    "    'enzyme_A', 'enzyme_B_by_disease_stage_diff_mean', 'blood_treatment_interaction', \n",
    "    'blood_cells_by_disease_stage_diff_mean', 'enzyme_ratio', 'behavior_per_age', \n",
    "    'lipid_index_qbin', 'treatment'\n",
    "]\n",
    "\n",
    "final_params_xgb = {\n",
    "    'n_estimators': 788, 'learning_rate': 0.06238066751566709, 'max_depth': 6,\n",
    "    'min_child_weight': 1, 'gamma': 0.3960858015669083, 'subsample': 0.9214205395769955,\n",
    "    'colsample_bytree': 0.5491320112459366, 'reg_alpha': 0.002669920381142417,\n",
    "    'reg_lambda': 0.015444177908340267,\n",
    "    'objective': 'multi:softprob', 'eval_metric': 'mlogloss',\n",
    "    'tree_method': 'hist', 'random_state': RANDOM_STATE, 'n_jobs': -1\n",
    "}\n",
    "\n",
    "# [LightGBM]\n",
    "LGBM_FEATS_SELECTED = [\n",
    "    'obs_days', 'clot_time', 'age', 'chem02_trace_ratio', 'lipid_blood_ratio', \n",
    "    'disease_velocity', 'trace_metal', 'chem01_treatment_interaction', \n",
    "    'blood_treatment_interaction', 'health_per_stage', 'trace_treatment_interaction', \n",
    "    'chem_02', 'diff_chem', 'diff_enzyme', 'enzyme_B_by_age_group_diff_mean', \n",
    "    'sum_enzyme', 'enzyme_A_by_gender_zscore', 'log_obs_days', 'obs_days_squared'\n",
    "]\n",
    "\n",
    "final_params_lgb = {\n",
    "    'n_estimators': 912, 'learning_rate': 0.01090933368427726, 'num_leaves': 193,\n",
    "    'max_depth': 14, 'min_child_samples': 51, 'subsample': 0.5933220031847126,\n",
    "    'colsample_bytree': 0.5036928973563994, 'reg_alpha': 0.008835424632453443,\n",
    "    'reg_lambda': 0.07034822683895961,\n",
    "    'objective': 'multiclass', 'random_state': RANDOM_STATE, 'n_jobs': -1, 'verbosity': -1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c53bba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš€ [XGBoost] ì¬ê²€ì¦ ì‹œì‘ (41ê°œ í›„ë³´)...\n",
      "[1/41] + chem_01_range                  | Score: 0.47901 (â–² 0.47901)\n",
      "[2/41] + chem01_trace_combo             | Score: 0.55180 (â–² 0.07280)\n",
      "[3/41] + chem_01                        | Score: 0.56177 (â–² 0.00997)\n",
      "[4/41] + symptom_count                  | Score: 0.59189 (â–² 0.03012)\n",
      "[5/41] + clot_time_qbin                 | Score: 0.60616 (â–² 0.01427)\n",
      "[6/41] + symptom_severity               | Score: 0.61015 (â–² 0.00399)\n",
      "[7/41] - health_per_stage               | Score: 0.60850 (Pass)\n",
      "[8/41] + obs_days                       | Score: 0.64084 (â–² 0.03069)\n",
      "[9/41] - swelling_ord                   | Score: 0.64081 (Pass)\n",
      "[10/41] + gender_int                     | Score: 0.64193 (â–² 0.00109)\n",
      "[11/41] + age                            | Score: 0.65896 (â–² 0.01703)\n",
      "[12/41] + lipid_index_by_gender_ratio_mean | Score: 0.66159 (â–² 0.00262)\n",
      "[13/41] - chem_01_qbin                   | Score: 0.65926 (Pass)\n",
      "[14/41] + enzyme_B_by_disease_stage_zscore | Score: 0.66651 (â–² 0.00492)\n",
      "[15/41] + blood_cells_by_gender_zscore   | Score: 0.67722 (â–² 0.01071)\n",
      "[16/41] + enzyme_A_by_gender_diff_mean   | Score: 0.68259 (â–² 0.00537)\n",
      "[17/41] + chem01_treatment_interaction   | Score: 0.68662 (â–² 0.00403)\n",
      "[18/41] - deadly_combo                   | Score: 0.68632 (Pass)\n",
      "[19/41] + enzyme_A_by_disease_stage_ratio_mean | Score: 0.69322 (â–² 0.00661)\n",
      "[20/41] - enzyme_B                       | Score: 0.69200 (Pass)\n",
      "[21/41] - sum_enzyme                     | Score: 0.69009 (Pass)\n",
      "[22/41] + chem01_chem02_combo            | Score: 0.69402 (â–² 0.00079)\n",
      "[23/41] + sum_chem                       | Score: 0.69539 (â–² 0.00137)\n",
      "[24/41] + blood_cells_by_gender_ratio_mean | Score: 0.69627 (â–² 0.00088)\n",
      "[25/41] - trace_treatment_interaction    | Score: 0.69444 (Pass)\n",
      "[26/41] - enzyme_B_by_age_group_ratio_mean | Score: 0.69611 (Pass)\n",
      "[27/41] - lipid_index_by_disease_stage_ratio_mean | Score: 0.69290 (Pass)\n",
      "[28/41] + blood_cells_by_age_group_diff_mean | Score: 0.69639 (â–² 0.00012)\n",
      "[29/41] + enzyme_A                       | Score: 0.69741 (â–² 0.00102)\n",
      "[30/41] + enzyme_B_by_disease_stage_diff_mean | Score: 0.69807 (â–² 0.00065)\n",
      "[31/41] + blood_treatment_interaction    | Score: 0.70367 (â–² 0.00561)\n",
      "[32/41] - blood_cells_by_disease_stage_diff_mean | Score: 0.70167 (Pass)\n",
      "[33/41] + enzyme_ratio                   | Score: 0.70396 (â–² 0.00029)\n",
      "[34/41] - behavior_per_age               | Score: 0.69733 (Pass)\n",
      "[35/41] + lipid_index_qbin               | Score: 0.70412 (â–² 0.00016)\n",
      "[36/41] - treatment                      | Score: 0.69954 (Pass)\n",
      "[37/41] - immune_per_behavior            | Score: 0.69785 (Pass)\n",
      "[38/41] - clot_per_cell                  | Score: 0.70094 (Pass)\n",
      "[39/41] - lipid_per_protein              | Score: 0.70161 (Pass)\n",
      "[40/41] - trace_density                  | Score: 0.70215 (Pass)\n",
      "[41/41] - behavior_treatment             | Score: 0.70076 (Pass)\n",
      "\n",
      "âœ… [XGBoost] ìµœì¢… ê²°ê³¼: 24ê°œ ì„ íƒë¨ (ê¸°ì¡´ 41ê°œ)\n",
      "ğŸ† Best mAP: 0.704122\n",
      "Selected Features: ['chem_01_range', 'chem01_trace_combo', 'chem_01', 'symptom_count', 'clot_time_qbin', 'symptom_severity', 'obs_days', 'gender_int', 'age', 'lipid_index_by_gender_ratio_mean', 'enzyme_B_by_disease_stage_zscore', 'blood_cells_by_gender_zscore', 'enzyme_A_by_gender_diff_mean', 'chem01_treatment_interaction', 'enzyme_A_by_disease_stage_ratio_mean', 'chem01_chem02_combo', 'sum_chem', 'blood_cells_by_gender_ratio_mean', 'blood_cells_by_age_group_diff_mean', 'enzyme_A', 'enzyme_B_by_disease_stage_diff_mean', 'blood_treatment_interaction', 'enzyme_ratio', 'lipid_index_qbin']\n",
      "\n",
      "ğŸš€ [LightGBM] ì¬ê²€ì¦ ì‹œì‘ (24ê°œ í›„ë³´)...\n",
      "[1/24] + obs_days                       | Score: 0.55887 (â–² 0.55887)\n",
      "[2/24] + clot_time                      | Score: 0.58244 (â–² 0.02357)\n",
      "[3/24] - age                            | Score: 0.58016 (Pass)\n",
      "[4/24] + chem02_trace_ratio             | Score: 0.58364 (â–² 0.00120)\n",
      "[5/24] + lipid_blood_ratio              | Score: 0.60242 (â–² 0.01877)\n",
      "[6/24] + disease_velocity               | Score: 0.61036 (â–² 0.00795)\n",
      "[7/24] + trace_metal                    | Score: 0.63552 (â–² 0.02516)\n",
      "[8/24] + chem01_treatment_interaction   | Score: 0.64263 (â–² 0.00711)\n",
      "[9/24] + blood_treatment_interaction    | Score: 0.65480 (â–² 0.01217)\n",
      "[10/24] + health_per_stage               | Score: 0.65638 (â–² 0.00158)\n",
      "[11/24] + trace_treatment_interaction    | Score: 0.65711 (â–² 0.00072)\n",
      "[12/24] + chem_02                        | Score: 0.66199 (â–² 0.00489)\n",
      "[13/24] + diff_chem                      | Score: 0.66447 (â–² 0.00248)\n",
      "[14/24] + diff_enzyme                    | Score: 0.67401 (â–² 0.00954)\n",
      "[15/24] + enzyme_B_by_age_group_diff_mean | Score: 0.68132 (â–² 0.00731)\n",
      "[16/24] + sum_enzyme                     | Score: 0.68160 (â–² 0.00028)\n",
      "[17/24] + enzyme_A_by_gender_zscore      | Score: 0.68434 (â–² 0.00273)\n",
      "[18/24] - log_obs_days                   | Score: 0.68337 (Pass)\n",
      "[19/24] - obs_days_squared               | Score: 0.68283 (Pass)\n",
      "[20/24] - immune_per_behavior            | Score: 0.67475 (Pass)\n",
      "[21/24] - clot_per_cell                  | Score: 0.68360 (Pass)\n",
      "[22/24] - lipid_per_protein              | Score: 0.68215 (Pass)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 104\u001b[39m\n\u001b[32m    101\u001b[39m xgb_final_feats = run_forward_verification(\u001b[33m\"\u001b[39m\u001b[33mXGBoost\u001b[39m\u001b[33m\"\u001b[39m, xgb_model, XGB_FEATS_CANDIDATES, X_all, y)\n\u001b[32m    103\u001b[39m lgb_model = LGBMClassifier(**final_params_lgb)\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m lgb_final_feats = \u001b[43mrun_forward_verification\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mLightGBM\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlgb_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLGBM_FEATS_CANDIDATES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_all\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 71\u001b[39m, in \u001b[36mrun_forward_verification\u001b[39m\u001b[34m(model_name, model, candidate_feats, X, y)\u001b[39m\n\u001b[32m     69\u001b[39m fold_scores = []\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m train_idx, val_idx \u001b[38;5;129;01min\u001b[39;00m kf.split(X_subset, y):\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m     \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_subset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     72\u001b[39m     preds = model.predict_proba(X_subset[val_idx])\n\u001b[32m     74\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m preds.shape[\u001b[32m1\u001b[39m] > \u001b[32m2\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\abc01\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\lightgbm\\sklearn.py:1560\u001b[39m, in \u001b[36mLGBMClassifier.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001b[39m\n\u001b[32m   1557\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1558\u001b[39m             valid_sets.append((valid_x, \u001b[38;5;28mself\u001b[39m._le.transform(valid_y)))\n\u001b[32m-> \u001b[39m\u001b[32m1560\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1561\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1562\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1563\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1564\u001b[39m \u001b[43m    \u001b[49m\u001b[43minit_score\u001b[49m\u001b[43m=\u001b[49m\u001b[43minit_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1565\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalid_sets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1566\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1567\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_sample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1568\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_class_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_class_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1569\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_init_score\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_init_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1570\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_metric\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_metric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1571\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1572\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1573\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1574\u001b[39m \u001b[43m    \u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1575\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1576\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\abc01\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\lightgbm\\sklearn.py:1049\u001b[39m, in \u001b[36mLGBMModel.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001b[39m\n\u001b[32m   1046\u001b[39m evals_result: _EvalResultDict = {}\n\u001b[32m   1047\u001b[39m callbacks.append(record_evaluation(evals_result))\n\u001b[32m-> \u001b[39m\u001b[32m1049\u001b[39m \u001b[38;5;28mself\u001b[39m._Booster = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1050\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1051\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1052\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_estimators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1053\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalid_sets\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalid_sets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1054\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalid_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1055\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeval\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_metrics_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1056\u001b[39m \u001b[43m    \u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1057\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1058\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1060\u001b[39m \u001b[38;5;66;03m# This populates the property self.n_features_, the number of features in the fitted model,\u001b[39;00m\n\u001b[32m   1061\u001b[39m \u001b[38;5;66;03m# and so should only be set after fitting.\u001b[39;00m\n\u001b[32m   1062\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m   1063\u001b[39m \u001b[38;5;66;03m# The related property self._n_features_in, which populates self.n_features_in_,\u001b[39;00m\n\u001b[32m   1064\u001b[39m \u001b[38;5;66;03m# is set BEFORE fitting.\u001b[39;00m\n\u001b[32m   1065\u001b[39m \u001b[38;5;28mself\u001b[39m._n_features = \u001b[38;5;28mself\u001b[39m._Booster.num_feature()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\abc01\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\lightgbm\\engine.py:322\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(params, train_set, num_boost_round, valid_sets, valid_names, feval, init_model, keep_training_booster, callbacks)\u001b[39m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m callbacks_before_iter:\n\u001b[32m    311\u001b[39m     cb(\n\u001b[32m    312\u001b[39m         callback.CallbackEnv(\n\u001b[32m    313\u001b[39m             model=booster,\n\u001b[32m   (...)\u001b[39m\u001b[32m    319\u001b[39m         )\n\u001b[32m    320\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m322\u001b[39m \u001b[43mbooster\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfobj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    324\u001b[39m evaluation_result_list: List[_LGBM_BoosterEvalMethodResultType] = []\n\u001b[32m    325\u001b[39m \u001b[38;5;66;03m# check evaluation result.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\abc01\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\lightgbm\\basic.py:4155\u001b[39m, in \u001b[36mBooster.update\u001b[39m\u001b[34m(self, train_set, fobj)\u001b[39m\n\u001b[32m   4152\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.__set_objective_to_none:\n\u001b[32m   4153\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m LightGBMError(\u001b[33m\"\u001b[39m\u001b[33mCannot update due to null objective function.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   4154\u001b[39m _safe_call(\n\u001b[32m-> \u001b[39m\u001b[32m4155\u001b[39m     \u001b[43m_LIB\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLGBM_BoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4156\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4157\u001b[39m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mis_finished\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4158\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4159\u001b[39m )\n\u001b[32m   4160\u001b[39m \u001b[38;5;28mself\u001b[39m.__is_predicted_cur_iter = [\u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.__num_dataset)]\n\u001b[32m   4161\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m is_finished.value == \u001b[32m1\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# %% [ê²€ì¦] ì„ ì •ëœ í”¼ì²˜ ë¦¬ìŠ¤íŠ¸ ë‚´ì—ì„œ ì¬ìµœì í™” (Forward Selection)\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "NEW_ML_FEATS = [\n",
    "    'immune_per_behavior', 'clot_per_cell', 'lipid_per_protein', \n",
    "    'trace_density', 'behavior_treatment'\n",
    "]\n",
    "\n",
    "# 1. ê²€ì¦í•  í”¼ì²˜ ë¦¬ìŠ¤íŠ¸ ì •ì˜ (ì‚¬ìš©ìë‹˜ì´ ë½‘ì•„ë‘” ê²ƒ)\n",
    "XGB_FEATS_CANDIDATES = [\n",
    "    'chem_01_range', 'chem01_trace_combo', 'chem_01', 'symptom_count', 'clot_time_qbin', \n",
    "    'symptom_severity', 'health_per_stage', 'obs_days', 'swelling_ord', 'gender_int', \n",
    "    'age', 'lipid_index_by_gender_ratio_mean', 'chem_01_qbin', 'enzyme_B_by_disease_stage_zscore', \n",
    "    'blood_cells_by_gender_zscore', 'enzyme_A_by_gender_diff_mean', 'chem01_treatment_interaction', \n",
    "    'deadly_combo', 'enzyme_A_by_disease_stage_ratio_mean', 'enzyme_B', 'sum_enzyme', \n",
    "    'chem01_chem02_combo', 'sum_chem', 'blood_cells_by_gender_ratio_mean', \n",
    "    'trace_treatment_interaction', 'enzyme_B_by_age_group_ratio_mean', \n",
    "    'lipid_index_by_disease_stage_ratio_mean', 'blood_cells_by_age_group_diff_mean', \n",
    "    'enzyme_A', 'enzyme_B_by_disease_stage_diff_mean', 'blood_treatment_interaction', \n",
    "    'blood_cells_by_disease_stage_diff_mean', 'enzyme_ratio', 'behavior_per_age', \n",
    "    'lipid_index_qbin', 'treatment'\n",
    "] + NEW_ML_FEATS\n",
    "\n",
    "LGBM_FEATS_CANDIDATES = [\n",
    "    'obs_days', 'clot_time', 'age', 'chem02_trace_ratio', 'lipid_blood_ratio', \n",
    "    'disease_velocity', 'trace_metal', 'chem01_treatment_interaction', \n",
    "    'blood_treatment_interaction', 'health_per_stage', 'trace_treatment_interaction', \n",
    "    'chem_02', 'diff_chem', 'diff_enzyme', 'enzyme_B_by_age_group_diff_mean', \n",
    "    'sum_enzyme', 'enzyme_A_by_gender_zscore', 'log_obs_days', 'obs_days_squared'\n",
    "]+ NEW_ML_FEATS\n",
    "\n",
    "# 2. ì´ë¯¸ ì°¾ì€ ìµœì  íŒŒë¼ë¯¸í„° (ì´ê±¸ë¡œ ê²€ì¦í•´ì•¼ ì •í™•í•¨)\n",
    "final_params_xgb = {\n",
    "    'n_estimators': 788, 'learning_rate': 0.06238, 'max_depth': 6, 'min_child_weight': 1,\n",
    "    'gamma': 0.3961, 'subsample': 0.9214, 'colsample_bytree': 0.5491, 'reg_alpha': 0.0027,\n",
    "    'reg_lambda': 0.0154, 'objective': 'multi:softprob', 'eval_metric': 'mlogloss',\n",
    "    'tree_method': 'hist', 'random_state': 42, 'n_jobs': -1, 'verbosity': 0\n",
    "}\n",
    "\n",
    "final_params_lgb = {\n",
    "    'n_estimators': 912, 'learning_rate': 0.0109, 'num_leaves': 193, 'max_depth': 14,\n",
    "    'min_child_samples': 51, 'subsample': 0.5933, 'colsample_bytree': 0.5037,\n",
    "    'reg_alpha': 0.0088, 'reg_lambda': 0.0703, 'objective': 'multiclass',\n",
    "    'random_state': 42, 'n_jobs': -1, 'verbosity': -1\n",
    "}\n",
    "\n",
    "# 3. Forward Selection í•¨ìˆ˜\n",
    "def run_forward_verification(model_name, model, candidate_feats, X, y):\n",
    "    print(f\"\\nğŸš€ [{model_name}] ì¬ê²€ì¦ ì‹œì‘ ({len(candidate_feats)}ê°œ í›„ë³´)...\")\n",
    "    \n",
    "    # í›„ë³´ í”¼ì²˜ê°€ ì‹¤ì œ ë°ì´í„°ì— ìˆëŠ”ì§€ í™•ì¸\n",
    "    valid_candidates = [f for f in candidate_feats if f in X.columns]\n",
    "    \n",
    "    best_score = 0.0\n",
    "    best_subset = []\n",
    "    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    # ìˆœì„œëŠ” ê¸°ì¡´ ë¦¬ìŠ¤íŠ¸ ìˆœì„œëŒ€ë¡œ(ì¤‘ìš”ë„ ìˆœ) ì§„í–‰\n",
    "    for i, feature in enumerate(valid_candidates):\n",
    "        current_features = best_subset + [feature]\n",
    "        X_subset = X[current_features].values\n",
    "        \n",
    "        fold_scores = []\n",
    "        for train_idx, val_idx in kf.split(X_subset, y):\n",
    "            model.fit(X_subset[train_idx], y[train_idx])\n",
    "            preds = model.predict_proba(X_subset[val_idx])\n",
    "            \n",
    "            if preds.shape[1] > 2:\n",
    "                y_bin = label_binarize(y[val_idx], classes=np.unique(y))\n",
    "                score = average_precision_score(y_bin, preds, average=\"macro\")\n",
    "            else:\n",
    "                score = average_precision_score(y[val_idx], preds[:, 1], average=\"macro\")\n",
    "            fold_scores.append(score)\n",
    "            \n",
    "        mean_score = np.mean(fold_scores)\n",
    "        \n",
    "        if mean_score > best_score:\n",
    "            diff = mean_score - best_score\n",
    "            best_score = mean_score\n",
    "            best_subset.append(feature)\n",
    "            print(f\"[{i+1}/{len(valid_candidates)}] + {feature:<30} | Score: {mean_score:.5f} (â–² {diff:.5f})\")\n",
    "        else:\n",
    "            # ì ìˆ˜ ì•ˆ ì˜¤ë¥´ë©´ ê³¼ê°íˆ ì œì™¸ (Pruning)\n",
    "            print(f\"[{i+1}/{len(valid_candidates)}] - {feature:<30} | Score: {mean_score:.5f} (Pass)\")\n",
    "            pass\n",
    "            \n",
    "    print(f\"\\nâœ… [{model_name}] ìµœì¢… ê²°ê³¼: {len(best_subset)}ê°œ ì„ íƒë¨ (ê¸°ì¡´ {len(valid_candidates)}ê°œ)\")\n",
    "    print(f\"ğŸ† Best mAP: {best_score:.6f}\")\n",
    "    print(f\"Selected Features: {best_subset}\")\n",
    "    return best_subset\n",
    "\n",
    "# 4. ì‹¤í–‰\n",
    "# (X_all, yëŠ” ì „ì²˜ë¦¬ ì™„ë£Œëœ ìƒíƒœì—¬ì•¼ í•¨)\n",
    "xgb_model = XGBClassifier(**final_params_xgb)\n",
    "xgb_final_feats = run_forward_verification(\"XGBoost\", xgb_model, XGB_FEATS_CANDIDATES, X_all, y)\n",
    "\n",
    "lgb_model = LGBMClassifier(**final_params_lgb)\n",
    "lgb_final_feats = run_forward_verification(\"LightGBM\", lgb_model, LGBM_FEATS_CANDIDATES, X_all, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b70270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (6500, 24)\n",
      "Test shape:  (1405, 23)\n",
      "Preprocessing Start...\n",
      "Preprocessing Done.\n",
      "Applying Smart Scaling on 99 columns...\n",
      "ğŸš€ OOF Training Start...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 346\u001b[39m\n\u001b[32m    343\u001b[39m xgb_model = XGBClassifier(**final_params_xgb)\n\u001b[32m    344\u001b[39m lgb_model = LGBMClassifier(**final_params_lgb)\n\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m oof_train_xgb, oof_test_xgb = \u001b[43mget_oof_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_xgb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_xgb_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m=\u001b[49m\u001b[43mN_SPLITS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    347\u001b[39m oof_train_lgb, oof_test_lgb = get_oof_predictions(lgb_model, X_lgb, y, X_lgb_test, n_splits=N_SPLITS)\n\u001b[32m    348\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mOOF Done.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 336\u001b[39m, in \u001b[36mget_oof_predictions\u001b[39m\u001b[34m(model, X, y, X_test, n_splits)\u001b[39m\n\u001b[32m    333\u001b[39m X_v = X_val[val_idx]\n\u001b[32m    335\u001b[39m clf = clone(model)\n\u001b[32m--> \u001b[39m\u001b[32m336\u001b[39m \u001b[43mclf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_tr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    338\u001b[39m oof_train[val_idx] = clf.predict_proba(X_v)\n\u001b[32m    339\u001b[39m oof_test += clf.predict_proba(X_te_val) / n_splits\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\abc01\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\core.py:774\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    772\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    773\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m774\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\abc01\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\sklearn.py:1803\u001b[39m, in \u001b[36mXGBClassifier.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[39m\n\u001b[32m   1783\u001b[39m evals_result: EvalsLog = {}\n\u001b[32m   1784\u001b[39m train_dmatrix, evals = _wrap_evaluation_matrices(\n\u001b[32m   1785\u001b[39m     missing=\u001b[38;5;28mself\u001b[39m.missing,\n\u001b[32m   1786\u001b[39m     X=X,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1800\u001b[39m     feature_types=feature_types,\n\u001b[32m   1801\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1803\u001b[39m \u001b[38;5;28mself\u001b[39m._Booster = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1804\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1805\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1806\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1807\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1808\u001b[39m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1809\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1810\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1811\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1812\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1813\u001b[39m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1814\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1815\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1817\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m.objective):\n\u001b[32m   1818\u001b[39m     \u001b[38;5;28mself\u001b[39m.objective = params[\u001b[33m\"\u001b[39m\u001b[33mobjective\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\abc01\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\core.py:774\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    772\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    773\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m774\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\abc01\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:199\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(params, dtrain, num_boost_round, evals, obj, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[39m\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cb_container.before_iteration(bst, i, dtrain, evals):\n\u001b[32m    198\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m199\u001b[39m \u001b[43mbst\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miteration\u001b[49m\u001b[43m=\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfobj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cb_container.after_iteration(bst, i, dtrain, evals):\n\u001b[32m    201\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\abc01\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\core.py:2434\u001b[39m, in \u001b[36mBooster.update\u001b[39m\u001b[34m(self, dtrain, iteration, fobj)\u001b[39m\n\u001b[32m   2430\u001b[39m \u001b[38;5;28mself\u001b[39m._assign_dmatrix_features(dtrain)\n\u001b[32m   2432\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2433\u001b[39m     _check_call(\n\u001b[32m-> \u001b[39m\u001b[32m2434\u001b[39m         \u001b[43m_LIB\u001b[49m\u001b[43m.\u001b[49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2435\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle\u001b[49m\n\u001b[32m   2436\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2437\u001b[39m     )\n\u001b[32m   2438\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2439\u001b[39m     pred = \u001b[38;5;28mself\u001b[39m.predict(dtrain, output_margin=\u001b[38;5;28;01mTrue\u001b[39;00m, training=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ë°ì´í„°ì…‹ ì¤€ë¹„\n",
    "X_xgb = X_all[XGB_FEATS_SELECTED].copy()\n",
    "X_xgb_test = X_all_test[XGB_FEATS_SELECTED].copy()\n",
    "\n",
    "X_lgb = X_all[LGBM_FEATS_SELECTED].copy()\n",
    "X_lgb_test = X_all_test[LGBM_FEATS_SELECTED].copy()\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 6. OOF ìƒì„± ë° ëª¨ë¸ í•™ìŠµ\n",
    "# ----------------------------------------------------\n",
    "def get_oof_predictions(model, X, y, X_test, n_splits=5):\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_STATE)\n",
    "    oof_train = np.zeros((X.shape[0], 3))\n",
    "    oof_test = np.zeros((X_test.shape[0], 3))\n",
    "    \n",
    "    # .values í™•ì¸\n",
    "    X_val = X.values if hasattr(X, 'values') else X\n",
    "    X_te_val = X_test.values if hasattr(X_test, 'values') else X_test\n",
    "    \n",
    "    for train_idx, val_idx in skf.split(X_val, y):\n",
    "        X_tr, y_tr = X_val[train_idx], y[train_idx]\n",
    "        X_v = X_val[val_idx]\n",
    "        \n",
    "        clf = clone(model)\n",
    "        clf.fit(X_tr, y_tr)\n",
    "        \n",
    "        oof_train[val_idx] = clf.predict_proba(X_v)\n",
    "        oof_test += clf.predict_proba(X_te_val) / n_splits\n",
    "    return oof_train, oof_test\n",
    "\n",
    "print(\"ğŸš€ OOF Training Start...\")\n",
    "xgb_model = XGBClassifier(**final_params_xgb)\n",
    "lgb_model = LGBMClassifier(**final_params_lgb)\n",
    "\n",
    "oof_train_xgb, oof_test_xgb = get_oof_predictions(xgb_model, X_xgb, y, X_xgb_test, n_splits=N_SPLITS)\n",
    "oof_train_lgb, oof_test_lgb = get_oof_predictions(lgb_model, X_lgb, y, X_lgb_test, n_splits=N_SPLITS)\n",
    "print(\"OOF Done.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefac397",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------------------------------------------------\n",
    "# 7. ë©”íƒ€ ëª¨ë¸ (Stacking) & ê°€ì¤‘ì¹˜ ì•™ìƒë¸” (Weighted)\n",
    "# ----------------------------------------------------\n",
    "X_meta_train = np.hstack([oof_train_xgb, oof_train_lgb])\n",
    "X_meta_test = np.hstack([oof_test_xgb, oof_test_lgb])\n",
    "\n",
    "# (1) Stacking (Logistic Regression)\n",
    "# Meta Params from tuning: C=0.58855...\n",
    "meta_model = LogisticRegression(C=0.5885532096799948, solver='lbfgs', random_state=RANDOM_STATE, max_iter=1000)\n",
    "meta_model.fit(X_meta_train, y)\n",
    "pred_stacking = meta_model.predict_proba(X_meta_test)\n",
    "\n",
    "# (2) Weighted Blending\n",
    "# Weights from tuning: XGB(0.59) / LGB(0.41) (based on your previous logs/assumption)\n",
    "# ë˜ëŠ” ë­í¬ ì•™ìƒë¸”(0.75 / 0.25) ë“±\n",
    "W_XGB = 0.60\n",
    "W_LGB = 0.40\n",
    "pred_weighted = (oof_test_xgb * W_XGB) + (oof_test_lgb * W_LGB)\n",
    "\n",
    "# (3) Rank Ensemble\n",
    "def get_rank(proba):\n",
    "    ranks = np.zeros_like(proba)\n",
    "    for i in range(proba.shape[1]):\n",
    "        ranks[:, i] = rankdata(proba[:, i]) / len(proba)\n",
    "    return ranks\n",
    "\n",
    "# Rank Weights (Example: XGB 0.75, LGB 0.25 from log)\n",
    "R_XGB = 0.75\n",
    "R_LGB = 0.25\n",
    "rank_xgb = get_rank(oof_test_xgb)\n",
    "rank_lgb = get_rank(oof_test_lgb)\n",
    "pred_rank = (rank_xgb * R_XGB) + (rank_lgb * R_LGB)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22efab9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š ì•™ìƒë¸” ëª¨ë¸ë³„ CV MAP ì ìˆ˜ í‰ê°€ ì¤‘...\n",
      "\n",
      "ğŸ† [Stacking Ensemble] CV MAP: 0.690088\n",
      "âš–ï¸ [Weighted Ensemble] CV MAP: 0.700440\n",
      "ğŸ¥‡ [Rank Ensemble]     CV MAP: 0.698298\n",
      "\n",
      "âœ… í‰ê°€ ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# [ìˆ˜ì •] ìµœì¢… í‰ê°€ ë° MAP ì ìˆ˜ ì¶œë ¥ (ì™„ì „íŒ)\n",
    "# ====================================================\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from scipy.stats import rankdata\n",
    "import numpy as np\n",
    "\n",
    "# 1. mAP í‰ê°€ í•¨ìˆ˜ ì •ì˜ (ëˆ„ë½ëœ ë¶€ë¶„ ì¶”ê°€)\n",
    "def score_function(y_true, y_pred_proba):\n",
    "    # í´ë˜ìŠ¤ê°€ 3ê°œ ì´ìƒì¸ ê²½ìš° ì›-í•« ì¸ì½”ë”© í›„ mAP ê³„ì‚°\n",
    "    if y_pred_proba.shape[1] > 2:\n",
    "        y_bin = label_binarize(y_true, classes=np.unique(y_true))\n",
    "        return average_precision_score(y_bin, y_pred_proba, average=\"macro\")\n",
    "    else:\n",
    "        return average_precision_score(y_true, y_pred_proba[:, 1], average=\"macro\")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 7. ì•™ìƒë¸” ë° ì„±ëŠ¥ í‰ê°€ (CV MAP ê³„ì‚°)\n",
    "# ----------------------------------------------------\n",
    "print(\"\\nğŸ“Š ì•™ìƒë¸” ëª¨ë¸ë³„ CV MAP ì ìˆ˜ í‰ê°€ ì¤‘...\\n\")\n",
    "\n",
    "# ë©”íƒ€ ë°ì´í„° (XGB OOF + LGB OOF)\n",
    "if 'oof_train_xgb' in locals() and 'oof_train_lgb' in locals():\n",
    "    X_meta_train = np.hstack([oof_train_xgb, oof_train_lgb])\n",
    "\n",
    "    # (1) Stacking Ensemble í‰ê°€\n",
    "    meta_model = LogisticRegression(\n",
    "        C=0.5885532096799948, \n",
    "        solver='lbfgs', \n",
    "        random_state=42, \n",
    "        max_iter=1000\n",
    "    )\n",
    "\n",
    "    meta_oof_pred = cross_val_predict(\n",
    "        meta_model, \n",
    "        X_meta_train, \n",
    "        y, \n",
    "        cv=5, \n",
    "        method='predict_proba', \n",
    "        n_jobs=-1\n",
    "    )\n",
    "    score_stacking = score_function(y, meta_oof_pred)\n",
    "    print(f\"ğŸ† [Stacking Ensemble] CV MAP: {score_stacking:.6f}\")\n",
    "\n",
    "\n",
    "    # (2) Weighted Ensemble í‰ê°€\n",
    "    W_XGB = 0.60\n",
    "    W_LGB = 0.40\n",
    "\n",
    "    # Train OOFì— ê°€ì¤‘ì¹˜ ì ìš©\n",
    "    weighted_oof_pred = (oof_train_xgb * W_XGB) + (oof_train_lgb * W_LGB)\n",
    "    score_weighted = score_function(y, weighted_oof_pred)\n",
    "    print(f\"âš–ï¸ [Weighted Ensemble] CV MAP: {score_weighted:.6f}\")\n",
    "\n",
    "\n",
    "    # (3) Rank Ensemble í‰ê°€\n",
    "    R_XGB = 0.75\n",
    "    R_LGB = 0.25\n",
    "\n",
    "    def get_rank(proba):\n",
    "        ranks = np.zeros_like(proba)\n",
    "        for i in range(proba.shape[1]):\n",
    "            # ê° í´ë˜ìŠ¤ë³„ í™•ë¥ ì„ ìˆœìœ„(0~1)ë¡œ ë³€í™˜\n",
    "            ranks[:, i] = rankdata(proba[:, i]) / len(proba)\n",
    "        return ranks\n",
    "\n",
    "    rank_train_xgb = get_rank(oof_train_xgb)\n",
    "    rank_train_lgb = get_rank(oof_train_lgb)\n",
    "\n",
    "    rank_oof_pred = (rank_train_xgb * R_XGB) + (rank_train_lgb * R_LGB)\n",
    "    score_rank = score_function(y, rank_oof_pred)\n",
    "    print(f\"ğŸ¥‡ [Rank Ensemble]     CV MAP: {score_rank:.6f}\")\n",
    "\n",
    "    print(\"\\nâœ… í‰ê°€ ì™„ë£Œ!\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
